{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQcC3-m8Syea"
   },
   "source": [
    "# 1. Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_gWmBX9hSyef"
   },
   "outputs": [],
   "source": [
    "\n",
    "import keras\n",
    "from numpy import dstack, hstack\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM, GRU, SimpleRNN, Bidirectional\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TimeDistributed \n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import concatenate\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import tensorflow.keras as keras\n",
    "from numpy import dstack, hstack, mean, std\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, LSTM, GRU, SimpleRNN, Bidirectional, Input, BatchNormalization, ConvLSTM2D, Conv1D, Conv2D, Conv3D,  MaxPooling1D, AveragePooling1D,MaxPooling2D, MaxPooling3D, concatenate, GlobalAveragePooling1D, GlobalMaxPooling1D, TimeDistributed\n",
    "from  tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from  tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "import time\n",
    "import itertools\n",
    "import tensorflow.keras.layers as  Layer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "import time\n",
    "import pandas as pd\n",
    "import csv\n",
    "import spidev\n",
    "from numpy import interp\n",
    "from time import sleep\n",
    "import RPi.GPIO as GPIO\n",
    "import signal\n",
    "import datetime, time\n",
    "# from threading import _Timer\n",
    "import csv\n",
    "import termios,sys,tty, fcntl, os,select\n",
    "import threading\n",
    "from sklearn import preprocessing\n",
    "import cv2\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 MIT Multi-Head Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "class ScaledDotProductAttention(keras.layers.Layer):\n",
    "    r\"\"\"The attention layer that takes three inputs representing queries, keys and values.\n",
    "    \\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{Q K^T}{\\sqrt{d_k}}) V\n",
    "    See: https://arxiv.org/pdf/1706.03762.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 return_attention=False,\n",
    "                 history_only=False,\n",
    "                 **kwargs):\n",
    "        \"\"\"Initialize the layer.\n",
    "        :param return_attention: Whether to return attention weights.\n",
    "        :param history_only: Whether to only use history data.\n",
    "        :param kwargs: Arguments for parent class.\n",
    "        \"\"\"\n",
    "        super(ScaledDotProductAttention, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        self.history_only = history_only\n",
    "        self.intensity = self.attention = None\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'return_attention': self.return_attention,\n",
    "            'history_only': self.history_only,\n",
    "        }\n",
    "        base_config = super(ScaledDotProductAttention, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if isinstance(input_shape, list):\n",
    "            query_shape, key_shape, value_shape = input_shape\n",
    "        else:\n",
    "            query_shape = key_shape = value_shape = input_shape\n",
    "        output_shape = query_shape[:-1] + value_shape[-1:]\n",
    "        if self.return_attention:\n",
    "            attention_shape = query_shape[:2] + (key_shape[1],)\n",
    "            return [output_shape, attention_shape]\n",
    "        return output_shape\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        if isinstance(mask, list):\n",
    "            mask = mask[0]\n",
    "        if self.return_attention:\n",
    "            return [mask, None]\n",
    "        return mask\n",
    "\n",
    "    def call(self, inputs, mask=None, **kwargs):\n",
    "        if isinstance(inputs, list):\n",
    "            query, key, value = inputs\n",
    "        else:\n",
    "            query = key = value = inputs\n",
    "        if isinstance(mask, list):\n",
    "            mask = mask[1]\n",
    "        feature_dim = K.shape(query)[-1]\n",
    "        e = K.batch_dot(query, key, axes=2) / K.sqrt(K.cast(feature_dim, dtype=K.floatx()))\n",
    "        if self.history_only:\n",
    "            query_len, key_len = K.shape(query)[1], K.shape(key)[1]\n",
    "            indices = K.expand_dims(K.arange(0, key_len), axis=0)\n",
    "            upper = K.expand_dims(K.arange(0, query_len), axis=-1)\n",
    "            e -= 10000.0 * K.expand_dims(K.cast(indices > upper, K.floatx()), axis=0)\n",
    "        if mask is not None:\n",
    "            e -= 10000.0 * (1.0 - K.cast(K.expand_dims(mask, axis=-2), K.floatx()))\n",
    "        self.intensity = e\n",
    "        e = K.exp(e - K.max(e, axis=-1, keepdims=True))\n",
    "        self.attention = e / K.sum(e, axis=-1, keepdims=True)\n",
    "        v = K.batch_dot(self.attention, value)\n",
    "        if self.return_attention:\n",
    "            return [v, self.attention]\n",
    "        return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras as keras \n",
    "# from keras_self_attention import ScaledDotProductAttention\n",
    "\n",
    "\n",
    "class MultiHeadAttention(keras.layers.Layer):\n",
    "    \"\"\"Multi-head attention layer.\n",
    "    See: https://arxiv.org/pdf/1706.03762.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 head_num,\n",
    "                 activation='relu',\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer='glorot_normal',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 history_only=False,\n",
    "                 **kwargs):\n",
    "        \"\"\"Initialize the layer.\n",
    "        :param head_num: Number of heads.\n",
    "        :param activation: Activations for linear mappings.\n",
    "        :param use_bias: Whether to use bias term.\n",
    "        :param kernel_initializer: Initializer for linear mappings.\n",
    "        :param bias_initializer: Initializer for linear mappings.\n",
    "        :param kernel_regularizer: Regularizer for linear mappings.\n",
    "        :param bias_regularizer: Regularizer for linear mappings.\n",
    "        :param kernel_constraint: Constraints for linear mappings.\n",
    "        :param bias_constraint: Constraints for linear mappings.\n",
    "        :param history_only: Whether to only use history in attention layer.\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.head_num = head_num\n",
    "        self.activation = keras.activations.get(activation)\n",
    "        self.use_bias = use_bias\n",
    "        self.kernel_initializer = keras.initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = keras.initializers.get(bias_initializer)\n",
    "        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = keras.regularizers.get(bias_regularizer)\n",
    "        self.kernel_constraint = keras.constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = keras.constraints.get(bias_constraint)\n",
    "        self.history_only = history_only\n",
    "\n",
    "        self.Wq = self.Wk = self.Wv = self.Wo = None\n",
    "        self.bq = self.bk = self.bv = self.bo = None\n",
    "\n",
    "        self.intensity = self.attention = None\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'head_num': self.head_num,\n",
    "            'activation': keras.activations.serialize(self.activation),\n",
    "            'use_bias': self.use_bias,\n",
    "            'kernel_initializer': keras.initializers.serialize(self.kernel_initializer),\n",
    "            'bias_initializer': keras.initializers.serialize(self.bias_initializer),\n",
    "            'kernel_regularizer': keras.regularizers.serialize(self.kernel_regularizer),\n",
    "            'bias_regularizer': keras.regularizers.serialize(self.bias_regularizer),\n",
    "            'kernel_constraint': keras.constraints.serialize(self.kernel_constraint),\n",
    "            'bias_constraint': keras.constraints.serialize(self.bias_constraint),\n",
    "            'history_only': self.history_only,\n",
    "        }\n",
    "        base_config = super(MultiHeadAttention, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if isinstance(input_shape, list):\n",
    "            q, k, v = input_shape\n",
    "            return q[:-1] + (v[-1],)\n",
    "        return input_shape\n",
    "\n",
    "    def compute_mask(self, inputs, input_mask=None):\n",
    "        if isinstance(input_mask, list):\n",
    "            return input_mask[0]\n",
    "        return input_mask\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if isinstance(input_shape, list):\n",
    "            q, k, v = input_shape\n",
    "        else:\n",
    "            q = k = v = input_shape\n",
    "        feature_dim = int(v[-1])\n",
    "        if feature_dim % self.head_num != 0:\n",
    "            raise IndexError('Invalid head number %d with the given input dim %d' % (self.head_num, feature_dim))\n",
    "        self.Wq = self.add_weight(\n",
    "            shape=(int(q[-1]), feature_dim),\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            constraint=self.kernel_constraint,\n",
    "            name='%s_Wq' % self.name,\n",
    "        )\n",
    "        if self.use_bias:\n",
    "            self.bq = self.add_weight(\n",
    "                shape=(feature_dim,),\n",
    "                initializer=self.bias_initializer,\n",
    "                regularizer=self.bias_regularizer,\n",
    "                constraint=self.bias_constraint,\n",
    "                name='%s_bq' % self.name,\n",
    "            )\n",
    "        self.Wk = self.add_weight(\n",
    "            shape=(int(k[-1]), feature_dim),\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            constraint=self.kernel_constraint,\n",
    "            name='%s_Wk' % self.name,\n",
    "        )\n",
    "        if self.use_bias:\n",
    "            self.bk = self.add_weight(\n",
    "                shape=(feature_dim,),\n",
    "                initializer=self.bias_initializer,\n",
    "                regularizer=self.bias_regularizer,\n",
    "                constraint=self.bias_constraint,\n",
    "                name='%s_bk' % self.name,\n",
    "            )\n",
    "        self.Wv = self.add_weight(\n",
    "            shape=(int(v[-1]), feature_dim),\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            constraint=self.kernel_constraint,\n",
    "            name='%s_Wv' % self.name,\n",
    "        )\n",
    "        if self.use_bias:\n",
    "            self.bv = self.add_weight(\n",
    "                shape=(feature_dim,),\n",
    "                initializer=self.bias_initializer,\n",
    "                regularizer=self.bias_regularizer,\n",
    "                constraint=self.bias_constraint,\n",
    "                name='%s_bv' % self.name,\n",
    "            )\n",
    "        self.Wo = self.add_weight(\n",
    "            shape=(feature_dim, feature_dim),\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            constraint=self.kernel_constraint,\n",
    "            name='%s_Wo' % self.name,\n",
    "        )\n",
    "        if self.use_bias:\n",
    "            self.bo = self.add_weight(\n",
    "                shape=(feature_dim,),\n",
    "                initializer=self.bias_initializer,\n",
    "                regularizer=self.bias_regularizer,\n",
    "                constraint=self.bias_constraint,\n",
    "                name='%s_bo' % self.name,\n",
    "            )\n",
    "        super(MultiHeadAttention, self).build(input_shape)\n",
    "\n",
    "    @staticmethod\n",
    "    def _reshape_to_batches(x, head_num):\n",
    "        input_shape = K.shape(x)\n",
    "        batch_size, seq_len, feature_dim = input_shape[0], input_shape[1], input_shape[2]\n",
    "        head_dim = feature_dim // head_num\n",
    "        x = K.reshape(x, (batch_size, seq_len, head_num, head_dim))\n",
    "        x = K.permute_dimensions(x, [0, 2, 1, 3])\n",
    "        return K.reshape(x, (batch_size * head_num, seq_len, head_dim))\n",
    "\n",
    "    @staticmethod\n",
    "    def _reshape_attention_from_batches(x, head_num):\n",
    "        input_shape = K.shape(x)\n",
    "        batch_size, seq_len, feature_dim = input_shape[0], input_shape[1], input_shape[2]\n",
    "        x = K.reshape(x, (batch_size // head_num, head_num, seq_len, feature_dim))\n",
    "        return K.permute_dimensions(x, [0, 2, 1, 3])\n",
    "\n",
    "    @staticmethod\n",
    "    def _reshape_from_batches(x, head_num):\n",
    "        input_shape = K.shape(x)\n",
    "        batch_size, seq_len, feature_dim = input_shape[0], input_shape[1], input_shape[2]\n",
    "        x = K.reshape(x, (batch_size // head_num, head_num, seq_len, feature_dim))\n",
    "        x = K.permute_dimensions(x, [0, 2, 1, 3])\n",
    "        return K.reshape(x, (batch_size // head_num, seq_len, feature_dim * head_num))\n",
    "\n",
    "    @staticmethod\n",
    "    def _reshape_mask(mask, head_num):\n",
    "        if mask is None:\n",
    "            return mask\n",
    "        seq_len = K.shape(mask)[1]\n",
    "        mask = K.expand_dims(mask, axis=1)\n",
    "        mask = K.tile(mask, [1, head_num, 1])\n",
    "        return K.reshape(mask, (-1, seq_len))\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if isinstance(inputs, list):\n",
    "            q, k, v = inputs\n",
    "        else:\n",
    "            q = k = v = inputs\n",
    "        if isinstance(mask, list):\n",
    "            q_mask, k_mask, v_mask = mask\n",
    "        else:\n",
    "            q_mask = k_mask = v_mask = mask\n",
    "        q = K.dot(q, self.Wq)\n",
    "        k = K.dot(k, self.Wk)\n",
    "        v = K.dot(v, self.Wv)\n",
    "        if self.use_bias:\n",
    "            q += self.bq\n",
    "            k += self.bk\n",
    "            v += self.bv\n",
    "        if self.activation is not None:\n",
    "            q = self.activation(q)\n",
    "            k = self.activation(k)\n",
    "            v = self.activation(v)\n",
    "        scaled_dot_product_attention = ScaledDotProductAttention(\n",
    "            history_only=self.history_only,\n",
    "            name='%s-Attention' % self.name,\n",
    "        )\n",
    "        y = scaled_dot_product_attention(\n",
    "            inputs=[\n",
    "                self._reshape_to_batches(q, self.head_num),\n",
    "                self._reshape_to_batches(k, self.head_num),\n",
    "                self._reshape_to_batches(v, self.head_num),\n",
    "            ],\n",
    "            mask=[\n",
    "                self._reshape_mask(q_mask, self.head_num),\n",
    "                self._reshape_mask(k_mask, self.head_num),\n",
    "                self._reshape_mask(v_mask, self.head_num),\n",
    "            ],\n",
    "        )\n",
    "        self.intensity = self._reshape_attention_from_batches(scaled_dot_product_attention.intensity, self.head_num)\n",
    "        self.attention = self._reshape_attention_from_batches(scaled_dot_product_attention.attention, self.head_num)\n",
    "        y = self._reshape_from_batches(y, self.head_num)\n",
    "        y = K.dot(y, self.Wo)\n",
    "        if self.use_bias:\n",
    "            y += self.bo\n",
    "        if self.activation is not None:\n",
    "            y = self.activation(y)\n",
    "        # if TF_KERAS:\n",
    "        #     # Add shape information to tensor when using `tf.keras`\n",
    "        input_shape = [K.int_shape(q), K.int_shape(k), K.int_shape(v)]\n",
    "        output_shape = self.compute_output_shape(input_shape)\n",
    "        if output_shape[1] is not None:\n",
    "            output_shape = (-1,) + output_shape[1:]\n",
    "            y = K.reshape(y, output_shape)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EEX8kqJESyeg"
   },
   "source": [
    "# 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "14QG521wS5cs"
   },
   "outputs": [],
   "source": [
    "\n",
    "data_filenames = ['Training_data/0/data_0_22_v7.csv','Training_data/0/data_0_22_2_v7.csv','Training_data/0/data_0_121_s1_t1_1_v7.csv', 'Training_data/0/data_0_211_s1_t1_1_v7.csv', \n",
    "                  'Training_data/0/data_0_220_s1_t1_1_v7.csv', 'Training_data/0/data_0_221_s1_t1_1_v7.csv', \n",
    "                  'Training_data/1/data_1_011_021_v7.csv','Training_data/1/data_1_120_122_v7.csv', 'Training_data/1/data_1_021_s1_t1_1_v7.csv',\n",
    "                  'Training_data/1/data_1_120_s1_t1_1_v7.csv','Training_data/1/data_1_122_s1_t1_1_v7.csv', 'Training_data/1/data_1_021_s2_t1_1_v7.csv',\n",
    "                  'Training_data/2/data_2_22_v7.csv','Training_data/2/data_2_22_2_v7.csv','Training_data/2/data_2_121_s1_t1_1_v7.csv','Training_data/2/data_2_211_s1_t1_1_v7.csv',\n",
    "                  'Training_data/2/data_2_220_s1_t1_1_v7.csv','Training_data/2/data_2_221_s1_t1_1_v7.csv','Training_data/2/data_2_222_s1_t1_1_v7.csv',\n",
    "                  'Training_data/3/data_3_22_v7.csv','Training_data/3/data_3_22_2_v7.csv','Training_data/3/data_3_121_s1_t1_1_v7.csv','Training_data/3/data_3_121_s2_t1_1_v7.csv',\n",
    "                  'Training_data/3/data_3_220_s1_t1_1_v7.csv','Training_data/3/data_3_220_s2_t1_1_v7.csv','Training_data/3/data_3_221_s1_t1_1_v7.csv','Training_data/3/data_3_221_s2_t1_1_v7.csv',  \n",
    "                  'Training_data/4/data_4_221_v7.csv', 'Training_data/4/data_4_221_2_v7.csv','Training_data/4/data_4_121_s1_t1_1_v7.csv','Training_data/4/data_4_121_s2_t1_1_v7.csv',\n",
    "                  'Training_data/4/data_4_211_s1_t1_1_v7.csv', 'Training_data/4/data_4_211_s2_t1_1_v7.csv', 'Training_data/4/data_4_210_s1_t1_1_v7.csv','Training_data/4/data_4_210_s2_t1_1_v7.csv',\n",
    "                  'Training_data/4/data_4_220_s1_t1_1_v7.csv','Training_data/4/data_4_220_s2_t1_1_v7.csv','Training_data/4/data_4_221_s1_t1_1_v7.csv','Training_data/4/data_4_221_s2_t1_1_v7.csv',\n",
    "                  'Training_data/5/data_5_221_v7.csv', 'Training_data/5/data_5_221_2_v7.csv', 'Training_data/5/data_5_121_s1_t1_1_v7.csv','Training_data/5/data_5_121_s1_t2_1_v7.csv',\n",
    "                  'Training_data/5/data_5_220_s1_t1_1_v7.csv','Training_data/5/data_5_220_s1_t2_1_v7.csv','Training_data/5/data_5_221_s1_t1_1_v7.csv','Training_data/5/data_5_221_s1_t2_1_v7.csv',\n",
    "                  'Training_data/6/data_6_121_v7.csv', 'Training_data/6/data_6_221_220_v7.csv', 'Training_data/6/data_6_121_s1_t1_1_v7.csv','Training_data/6/data_6_221_s1_t1_1_v7.csv',\n",
    "                  'Training_data/7/data_7_221_v7.csv','Training_data/7/data_7_221_3_v7.csv', 'Training_data/7/data_7_221_2_v7.csv', 'Training_data/7/data_7_121_s1_t1_1_v7.csv',\n",
    "                  'Training_data/7/data_7_121_s2_t1_1_v7.csv','Training_data/7/data_7_121_s3_t1_1_v7.csv', 'Training_data/7/data_7_221_s1_t1_1_v7.csv',\n",
    "                  'Training_data/7/data_7_221_s2_t1_1_v7.csv','Training_data/7/data_7_221_s3_t1_1_v7.csv',   \n",
    "                  'Training_data/8/data_8_221_v7.csv', 'Training_data/8/data_8_221_3_v7.csv', 'Training_data/8/data_8_121_s1_t1_1_v7.csv','Training_data/8/data_8_121_s1_t2_1_v7.csv',\n",
    "                  'Training_data/8/data_8_121_s1_t3_1_v7.csv', 'Training_data/8/data_8_220_s1_t1_1_v7.csv','Training_data/8/data_8_220_s1_t2_1_v7.csv','Training_data/8/data_8_220_s1_t3_1_v7.csv',\n",
    "                  'Training_data/8/data_8_221_s1_t1_1_v7.csv','Training_data/8/data_8_221_s1_t2_1_v7.csv','Training_data/8/data_8_221_s1_t3_1_v7.csv','Training_data/8/data_8_221_s1_t4_1_v7.csv',\n",
    "                  'Training_data/8/data_8_221_s1_t5_1_v7.csv','Training_data/8/data_8_121_s1_t4_1_v7.csv',\n",
    "                  'Training_data/9/data_9_120_v7.csv', 'Training_data/9/data_9_221_220_v7.csv','Training_data/9/data_9_221_220_3_v7.csv',\n",
    "                  'Training_data/9/data_9_120_s1_t1_1_v7.csv','Training_data/9/data_9_121_s1_t1_1_v7.csv','Training_data/9/data_9_220_s1_t1_1_v7.csv','Training_data/9/data_9_221_s1_t1_1_v7.csv']\n",
    "\n",
    "\n",
    "label_filenames = ['Training_data/0/data_0_22_v7.csv_label_v1.txt','Training_data/0/data_0_22_2_v7.csv_label_v1.txt','Training_data/0/data_0_121_s1_t1_1_v7.csv_label_v1.txt', \n",
    "                    'Training_data/0/data_0_211_s1_t1_1_v7.csv_label_v1.txt', 'Training_data/0/data_0_220_s1_t1_1_v7.csv_label_v1.txt', 'Training_data/0/data_0_221_s1_t1_1_v7.csv_label_v1.txt', \n",
    "                  'Training_data/1/data_1_011_021_v7.csv_label_v1.txt','Training_data/1/data_1_120_122_v7.csv_label_v1.txt', 'Training_data/1/data_1_021_s1_t1_1_v7.csv_label_v1.txt',\n",
    "                  'Training_data/1/data_1_120_s1_t1_1_v7.csv_label_v1.txt','Training_data/1/data_1_122_s1_t1_1_v7.csv_label_v1.txt','Training_data/1/data_1_021_s2_t1_1_v7.csv_label_v1.txt',\n",
    "                  'Training_data/2/data_2_22_v7.csv_label_v1.txt','Training_data/2/data_2_22_2_v7.csv_label_v1.txt','Training_data/2/data_2_121_s1_t1_1_v7.csv_label_v1.txt','Training_data/2/data_2_211_s1_t1_1_v7.csv_label_v1.txt',\n",
    "                  'Training_data/2/data_2_220_s1_t1_1_v7.csv_label_v1.txt','Training_data/2/data_2_221_s1_t1_1_v7.csv_label_v1.txt','Training_data/2/data_2_222_s1_t1_1_v7.csv_label_v1.txt',\n",
    "                  'Training_data/3/data_3_22_v7.csv_label_v1.txt','Training_data/3/data_3_22_2_v7.csv_label_v1.txt','Training_data/3/data_3_121_s1_t1_1_v7.csv_label_v1.txt','Training_data/3/data_3_121_s2_t1_1_v7.csv_label_v1.txt',\n",
    "                  'Training_data/3/data_3_220_s1_t1_1_v7.csv_label_v1.txt','Training_data/3/data_3_220_s2_t1_1_v7.csv_label_v1.txt','Training_data/3/data_3_221_s1_t1_1_v7.csv_label_v1.txt','Training_data/3/data_3_221_s2_t1_1_v7.csv_label_v1.txt',  \n",
    "                  'Training_data/4/data_4_221_v7.csv_label_v1.txt', 'Training_data/4/data_4_221_2_v7.csv_label_v1.txt','Training_data/4/data_4_121_s1_t1_1_v7.csv_label_v1.txt','Training_data/4/data_4_121_s2_t1_1_v7.csv_label_v1.txt',\n",
    "                  'Training_data/4/data_4_211_s1_t1_1_v7.csv_label_v1.txt', 'Training_data/4/data_4_211_s2_t1_1_v7.csv_label_v1.txt', 'Training_data/4/data_4_210_s1_t1_1_v7.csv_label_v1.txt','Training_data/4/data_4_210_s2_t1_1_v7.csv_label_v1.txt',\n",
    "                  'Training_data/4/data_4_220_s1_t1_1_v7.csv_label_v1.txt','Training_data/4/data_4_220_s2_t1_1_v7.csv_label_v1.txt','Training_data/4/data_4_221_s1_t1_1_v7.csv_label_v1.txt','Training_data/4/data_4_221_s2_t1_1_v7.csv_label_v1.txt',\n",
    "                  'Training_data/5/data_5_221_v7.csv_label_v1.txt', 'Training_data/5/data_5_221_2_v7.csv_label_v1.txt', 'Training_data/5/data_5_121_s1_t1_1_v7.csv_label_v1.txt','Training_data/5/data_5_121_s1_t2_1_v7.csv_label_v1.txt',\n",
    "                  'Training_data/5/data_5_220_s1_t1_1_v7.csv_label_v1.txt','Training_data/5/data_5_220_s1_t2_1_v7.csv_label_v1.txt','Training_data/5/data_5_221_s1_t1_1_v7.csv_label_v1.txt','Training_data/5/data_5_221_s1_t2_1_v7.csv_label_v1.txt',\n",
    "                  'Training_data/6/data_6_121_v7.csv_label_v1.txt', 'Training_data/6/data_6_221_220_v7.csv_label_v1.txt', 'Training_data/6/data_6_121_s1_t1_1_v7.csv_label_v1.txt','Training_data/6/data_6_221_s1_t1_1_v7.csv_label_v1.txt',\n",
    "                  'Training_data/7/data_7_221_v7.csv_label_v1.txt','Training_data/7/data_7_221_3_v7.csv_label_v1.txt', 'Training_data/7/data_7_221_2_v7.csv_label_v1.txt', 'Training_data/7/data_7_121_s1_t1_1_v7.csv_label_v1.txt',\n",
    "                  'Training_data/7/data_7_121_s2_t1_1_v7.csv_label_v1.txt','Training_data/7/data_7_121_s3_t1_1_v7.csv_label_v1.txt', 'Training_data/7/data_7_221_s1_t1_1_v7.csv_label_v1.txt',\n",
    "                  'Training_data/7/data_7_221_s2_t1_1_v7.csv_label_v1.txt','Training_data/7/data_7_221_s3_t1_1_v7.csv_label_v1.txt',   \n",
    "                  'Training_data/8/data_8_221_v7.csv_label_v1.txt', 'Training_data/8/data_8_221_3_v7.csv_label_v1.txt', 'Training_data/8/data_8_121_s1_t1_1_v7.csv_label_v1.txt','Training_data/8/data_8_121_s1_t2_1_v7.csv_label_v1.txt',\n",
    "                  'Training_data/8/data_8_121_s1_t3_1_v7.csv_label_v1.txt', 'Training_data/8/data_8_220_s1_t1_1_v7.csv_label_v1.txt','Training_data/8/data_8_220_s1_t2_1_v7.csv_label_v1.txt','Training_data/8/data_8_220_s1_t3_1_v7.csv_label_v1.txt',\n",
    "                  'Training_data/8/data_8_221_s1_t1_1_v7.csv_label_v1.txt','Training_data/8/data_8_221_s1_t2_1_v7.csv_label_v1.txt','Training_data/8/data_8_221_s1_t3_1_v7.csv_label_v1.txt',\n",
    "                  'Training_data/8/data_8_221_s1_t4_1_v7.csv_label_v1.txt','Training_data/8/data_8_221_s1_t5_1_v7.csv_label_v1.txt','Training_data/8/data_8_121_s1_t4_1_v7.csv_label_v1.txt',\n",
    "                  'Training_data/9/data_9_120_v7.csv_label_v1.txt', 'Training_data/9/data_9_221_220_v7.csv_label_v1.txt','Training_data/9/data_9_221_220_3_v7.csv_label_v1.txt',\n",
    "                  'Training_data/9/data_9_120_s1_t1_1_v7.csv_label_v1.txt','Training_data/9/data_9_121_s1_t1_1_v7.csv_label_v1.txt','Training_data/9/data_9_220_s1_t1_1_v7.csv_label_v1.txt','Training_data/9/data_9_221_s1_t1_1_v7.csv_label_v1.txt']\n",
    "\n",
    "data_test_filenames = [\n",
    "                        'Testing_data/jundaWu_v2/data_0_jundaWu_v7.csv','Testing_data/jundaWu_v2/data_1_jundaWu_v7.csv','Testing_data/jundaWu_v2/data_2_jundaWu_v7.csv',\n",
    "                        'Testing_data/jundaWu_v2/data_3_jundaWu_v7.csv','Testing_data/jundaWu_v2/data_4_jundaWu_v7.csv','Testing_data/jundaWu_v2/data_5_jundaWu_v7.csv',\n",
    "                        'Testing_data/jundaWu_v2/data_6_jundaWu_v7.csv','Testing_data/jundaWu_v2/data_7_jundaWu_v7.csv','Testing_data/jundaWu_v2/data_8_jundaWu_v7.csv',\n",
    "                        'Testing_data/jundaWu_v2/data_9_jundaWu_v7.csv',\n",
    "                        'Testing_data/shixunWu_v2/data_0_shiXun_v7.csv','Testing_data/shixunWu_v2/data_1_shiXun_v7.csv','Testing_data/shixunWu_v2/data_2_shiXun_v7.csv',\n",
    "                        'Testing_data/shixunWu_v2/data_3_shiXun_v7.csv','Testing_data/shixunWu_v2/data_4_shiXun_v7.csv','Testing_data/shixunWu_v2/data_5_shiXun_v7.csv',\n",
    "                        'Testing_data/shixunWu_v2/data_6_shiXun_v7.csv','Testing_data/shixunWu_v2/data_7_shiXun_v7.csv','Testing_data/shixunWu_v2/data_8_shiXun_v7.csv',\n",
    "                        'Testing_data/shixunWu_v2/data_9_shiXun_v7.csv',\n",
    "                        'Testing_data/xinyunXu/data_0_XinyunXu_v7.csv','Testing_data/xinyunXu/data_1_XinyunXu_v7.csv','Testing_data/xinyunXu/data_2_XinyunXu_v7.csv',\n",
    "                        'Testing_data/xinyunXu/data_3_XinyunXu_v7.csv','Testing_data/xinyunXu/data_4_XinyunXu_v7.csv','Testing_data/xinyunXu/data_5_XinyunXu_v7.csv',\n",
    "                        'Testing_data/xinyunXu/data_6_XinyunXu_v7.csv','Testing_data/xinyunXu/data_7_XinyunXu_v7.csv','Testing_data/xinyunXu/data_8_XinyunXu_v7.csv',\n",
    "                        'Testing_data/xinyunXu/data_9_XinyunXu_v7.csv',\n",
    "                        'Testing_data/yuanhaoXie/data_0_xie_3_v7.csv','Testing_data/yuanhaoXie/data_1_xie_3_v7.csv','Testing_data/yuanhaoXie/data_2_xie_3_v7.csv',\n",
    "                        'Testing_data/yuanhaoXie/data_3_xie_3_v7.csv','Testing_data/yuanhaoXie/data_4_xie_3_v7.csv','Testing_data/yuanhaoXie/data_5_xie_3_v7.csv',\n",
    "                        'Testing_data/yuanhaoXie/data_6_xie_3_v7.csv','Testing_data/yuanhaoXie/data_7_xie_3_v7.csv','Testing_data/yuanhaoXie/data_8_xie_3_v7.csv',\n",
    "                        'Testing_data/yuanhaoXie/data_9_xie_3_v7.csv',\n",
    "                        'Testing_data/yuanhaoXie/data_0_xie_v7.csv','Testing_data/yuanhaoXie/data_1_xie_v7.csv','Testing_data/yuanhaoXie/data_2_xie_v7.csv',\n",
    "                        'Testing_data/yuanhaoXie/data_3_xie_v7.csv','Testing_data/yuanhaoXie/data_4_xie_v7.csv','Testing_data/yuanhaoXie/data_5_xie_v7.csv',\n",
    "                        'Testing_data/yuanhaoXie/data_6_xie_v7.csv','Testing_data/yuanhaoXie/data_7_xie_v7.csv','Testing_data/yuanhaoXie/data_8_xie_v7.csv',\n",
    "                        'Testing_data/yuanhaoXie/data_9_xie_v7.csv',\n",
    "                        'Testing_data/yuanhaoXie/data_0_xie_2_v7.csv', 'Testing_data/yuanhaoXie/data_3_xie_2_v7.csv',\n",
    "                        'Testing_data/zehangWu/data_0_ZehangWu_v7.csv', 'Testing_data/zehangWu/data_1_ZehangWu_v7.csv', 'Testing_data/zehangWu/data_2_ZehangWu_v7.csv',\n",
    "                        'Testing_data/zehangWu/data_3_ZehangWu_v7.csv', 'Testing_data/zehangWu/data_4_ZehangWu_v7.csv', 'Testing_data/zehangWu/data_5_ZehangWu_v7.csv',\n",
    "                        'Testing_data/zehangWu/data_6_ZehangWu_v7.csv', 'Testing_data/zehangWu/data_7_ZehangWu_v7.csv', 'Testing_data/zehangWu/data_8_ZehangWu_v7.csv',\n",
    "                        'Testing_data/zehangWu/data_9_ZehangWu_v7.csv',\n",
    "                        'Testing_data/liang/data_0_Liang_v7.csv','Testing_data/liang/data_1_Liang_v7.csv','Testing_data/liang/data_2_Liang_v7.csv',\n",
    "                        'Testing_data/liang/data_3_Liang_v7.csv','Testing_data/liang/data_4_Liang_v7.csv','Testing_data/liang/data_5_Liang_v7.csv',\n",
    "                        'Testing_data/liang/data_6_Liang_v7.csv','Testing_data/liang/data_7_Liang_v7.csv','Testing_data/liang/data_8_Liang_v7.csv',\n",
    "                        'Testing_data/liang/data_9_Liang_v7.csv',\n",
    "                        'Testing_data/shixunWu/data_0_shiXun_v7.csv', 'Testing_data/shixunWu/data_1_shiXun_v7.csv','Testing_data/shixunWu/data_2_shiXun_v7.csv',\n",
    "                        'Testing_data/shixunWu/data_3_shiXun_v7.csv','Testing_data/shixunWu/data_4_shiXun_v7.csv','Testing_data/shixunWu/data_5_shiXun_v7.csv',\n",
    "                        'Testing_data/shixunWu/data_6_shiXun_v7.csv','Testing_data/shixunWu/data_7_shiXun_v7.csv','Testing_data/shixunWu/data_8_shiXun_v7.csv',\n",
    "                        'Testing_data/shixunWu/data_9_shiXun_v7.csv'\n",
    "\n",
    "                        ]\n",
    "label_test_filenames = [\n",
    "                        'Testing_data/jundaWu_v2/data_0_jundaWu_v7.csv_label_v1.txt','Testing_data/jundaWu_v2/data_1_jundaWu_v7.csv_label_v1.txt','Testing_data/jundaWu_v2/data_2_jundaWu_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/jundaWu_v2/data_3_jundaWu_v7.csv_label_v1.txt','Testing_data/jundaWu_v2/data_4_jundaWu_v7.csv_label_v1.txt','Testing_data/jundaWu_v2/data_5_jundaWu_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/jundaWu_v2/data_6_jundaWu_v7.csv_label_v1.txt','Testing_data/jundaWu_v2/data_7_jundaWu_v7.csv_label_v1.txt','Testing_data/jundaWu_v2/data_8_jundaWu_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/jundaWu_v2/data_9_jundaWu_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/shixunWu_v2/data_0_shiXun_v7.csv_label_v1.txt','Testing_data/shixunWu_v2/data_1_shiXun_v7.csv_label_v1.txt','Testing_data/shixunWu_v2/data_2_shiXun_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/shixunWu_v2/data_3_shiXun_v7.csv_label_v1.txt','Testing_data/shixunWu_v2/data_4_shiXun_v7.csv_label_v1.txt','Testing_data/shixunWu_v2/data_5_shiXun_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/shixunWu_v2/data_6_shiXun_v7.csv_label_v1.txt','Testing_data/shixunWu_v2/data_7_shiXun_v7.csv_label_v1.txt','Testing_data/shixunWu_v2/data_8_shiXun_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/shixunWu_v2/data_9_shiXun_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/xinyunXu/data_0_XinyunXu_v7.csv_label_v1.txt','Testing_data/xinyunXu/data_1_XinyunXu_v7.csv_label_v1.txt','Testing_data/xinyunXu/data_2_XinyunXu_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/xinyunXu/data_3_XinyunXu_v7.csv_label_v1.txt','Testing_data/xinyunXu/data_4_XinyunXu_v7.csv_label_v1.txt','Testing_data/xinyunXu/data_5_XinyunXu_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/xinyunXu/data_6_XinyunXu_v7.csv_label_v1.txt','Testing_data/xinyunXu/data_7_XinyunXu_v7.csv_label_v1.txt','Testing_data/xinyunXu/data_8_XinyunXu_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/xinyunXu/data_9_XinyunXu_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/yuanhaoXie/data_0_xie_3_v7.csv_label_v1.txt','Testing_data/yuanhaoXie/data_1_xie_3_v7.csv_label_v1.txt','Testing_data/yuanhaoXie/data_2_xie_3_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/yuanhaoXie/data_3_xie_3_v7.csv_label_v1.txt','Testing_data/yuanhaoXie/data_4_xie_3_v7.csv_label_v1.txt','Testing_data/yuanhaoXie/data_5_xie_3_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/yuanhaoXie/data_6_xie_3_v7.csv_label_v1.txt','Testing_data/yuanhaoXie/data_7_xie_3_v7.csv_label_v1.txt','Testing_data/yuanhaoXie/data_8_xie_3_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/yuanhaoXie/data_9_xie_3_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/yuanhaoXie/data_0_xie_v7.csv_label_v1.txt','Testing_data/yuanhaoXie/data_1_xie_v7.csv_label_v1.txt','Testing_data/yuanhaoXie/data_2_xie_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/yuanhaoXie/data_3_xie_v7.csv_label_v1.txt','Testing_data/yuanhaoXie/data_4_xie_v7.csv_label_v1.txt','Testing_data/yuanhaoXie/data_5_xie_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/yuanhaoXie/data_6_xie_v7.csv_label_v1.txt','Testing_data/yuanhaoXie/data_7_xie_v7.csv_label_v1.txt','Testing_data/yuanhaoXie/data_8_xie_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/yuanhaoXie/data_9_xie_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/yuanhaoXie/data_0_xie_2_v7.csv_label_v1.txt', 'Testing_data/yuanhaoXie/data_3_xie_2_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/zehangWu/data_0_ZehangWu_v7.csv_label_v1.txt', 'Testing_data/zehangWu/data_1_ZehangWu_v7.csv_label_v1.txt', 'Testing_data/zehangWu/data_2_ZehangWu_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/zehangWu/data_3_ZehangWu_v7.csv_label_v1.txt', 'Testing_data/zehangWu/data_4_ZehangWu_v7.csv_label_v1.txt', 'Testing_data/zehangWu/data_5_ZehangWu_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/zehangWu/data_6_ZehangWu_v7.csv_label_v1.txt', 'Testing_data/zehangWu/data_7_ZehangWu_v7.csv_label_v1.txt', 'Testing_data/zehangWu/data_8_ZehangWu_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/zehangWu/data_9_ZehangWu_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/liang/data_0_Liang_v7.csv_label_v1.txt','Testing_data/liang/data_1_Liang_v7.csv_label_v1.txt','Testing_data/liang/data_2_Liang_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/liang/data_3_Liang_v7.csv_label_v1.txt','Testing_data/liang/data_4_Liang_v7.csv_label_v1.txt','Testing_data/liang/data_5_Liang_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/liang/data_6_Liang_v7.csv_label_v1.txt','Testing_data/liang/data_7_Liang_v7.csv_label_v1.txt','Testing_data/liang/data_8_Liang_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/liang/data_9_Liang_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/shixunWu/data_0_shiXun_v7.csv_label_v1.txt', 'Testing_data/shixunWu/data_1_shiXun_v7.csv_label_v1.txt','Testing_data/shixunWu/data_2_shiXun_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/shixunWu/data_3_shiXun_v7.csv_label_v1.txt','Testing_data/shixunWu/data_4_shiXun_v7.csv_label_v1.txt','Testing_data/shixunWu/data_5_shiXun_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/shixunWu/data_6_shiXun_v7.csv_label_v1.txt','Testing_data/shixunWu/data_7_shiXun_v7.csv_label_v1.txt','Testing_data/shixunWu/data_8_shiXun_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/shixunWu/data_9_shiXun_v7.csv_label_v1.txt'\n",
    "\n",
    "                        ]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KAB_xh0JTJJU"
   },
   "outputs": [],
   "source": [
    "def create_label_frame(file_name):\n",
    "  label_frame = pd.DataFrame(columns = ['label'])\n",
    "  for i, file in enumerate(file_name):\n",
    "    data = pd.read_csv(file, names = ['label'])\n",
    "    label_frame = pd.concat([label_frame, data], axis = 0, ignore_index = True)\n",
    "  return label_frame\n",
    "\n",
    "def create_data_frame(file_name):\n",
    "  data_frame = []\n",
    "  for i, file in enumerate(file_name):\n",
    "    data = pd.read_csv(file)\n",
    "    data_frame.append (data)\n",
    "  data_frame = pd.concat(data_frame, join = 'outer')\n",
    "  return data_frame\n",
    "\n",
    "def data_3D_transfer(photodiode_arr, data_frame):\n",
    "  data_3D = list()\n",
    "  for i, photodiode_name in enumerate(photodiode_arr):\n",
    "    data_3D.append(data_frame[photodiode_name].values.reshape(-1, ))  \n",
    "  data_3D = dstack(data_3D)\n",
    "  data_3D = data_3D.reshape((-1, 300, 9))\n",
    "  return data_3D\n",
    "\n",
    "def load_dataset():\n",
    "                                         \n",
    "  photodiode_arr = ['6', '1', '2', '3', '9', '8', '7', '4', '5']\n",
    "  label_frame = create_label_frame(label_filenames)\n",
    "  label_test_frame = create_label_frame(label_test_filenames)\n",
    "  data_frame = create_data_frame(data_filenames)\n",
    "  data_test_frame = create_data_frame(data_test_filenames)\n",
    "  data_3D = data_3D_transfer(photodiode_arr, data_frame)\n",
    "  data_test_3D = data_3D_transfer(photodiode_arr, data_test_frame)\n",
    "  data_y = label_frame\n",
    "  data_test_y = label_test_frame\n",
    "  data_y = to_categorical(data_y,num_classes=10)\n",
    "  print('data_y : ', len(data_y))\n",
    "  print('data_test_y : ', len(data_test_y))\n",
    "  data_test_y = to_categorical(data_test_y,num_classes=10)\n",
    "  # Xtra,Xval,Ytra,Yval = train_test_split(data_3D,data_y,test_size=0.2,shuffle=True)\n",
    "  return  data_3D, data_y, data_test_3D,  data_test_y\n",
    "  # return  Xtra,Ytra, data_test_3D,  data_test_y, Xval,Yval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71agi6z2Syeh"
   },
   "source": [
    "# 3. Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xiUFWeJGTlYw"
   },
   "outputs": [],
   "source": [
    "learning_rate = [0.001]\n",
    "verbose, epochs  = 0, 20\n",
    "Batch_size = [32]\n",
    "\n",
    "n_steps, n_length = 60, 5\n",
    "num_filters = [32]\n",
    "num_kernels = [3]\n",
    "num_lstm_Neurons = [1024]\n",
    "num_fcn_Neurons = [1024]\n",
    "Attention_Neurons = [1]\n",
    "\n",
    "\n",
    "n_steps_Ex, n_length_Ex = 60, 5 \n",
    "num_filters_Ex = [64]\n",
    "num_kernels_Ex = [2]\n",
    "num_heads = [2]\n",
    "num_lstm_Neurons_Ex = [512]\n",
    "num_fcn_Neurons_Ex = [1024]\n",
    "Attention_Neurons_Ex = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_head, att_neuron, num_lstm_Neuron, num_fcn_Neuron, num_filter,num_kernel, n_timesteps, n_steps, n_length, n_features, n_outputs):\n",
    "  inputs = Input(shape = (n_steps, n_length, n_features))\n",
    "  TDCnn = TimeDistributed(Conv1D(filters= num_filter, kernel_size=num_kernel,  activation = 'relu'), name = 'TDCNN')(inputs)\n",
    "  Pool = TimeDistributed(MaxPooling1D(pool_size =num_kernel))(TDCnn)  \n",
    "  dp = TimeDistributed(Dropout(0.5))(Pool)\n",
    "  flat = TimeDistributed(Flatten())(dp)\n",
    "  gru = Bidirectional(GRU(num_lstm_Neuron, return_sequences = True))(flat)   \n",
    "  MulselfAtt = MultiHeadAttention(head_num=num_head,name='Multi-Head')([gru, gru, gru])\n",
    "  # dp1 = Dropout(0.5)(MulselfAtt)  \n",
    "  fla = GlobalAveragePooling1D()(MulselfAtt)\n",
    "  # gru2 = Bidirectional(GRU(num_lstm_Neuron, return_sequences=False))(MulselfAtt)\n",
    "  # dp1 = Dropout(0.5)(fla)\n",
    "  # den1 = Dense(num_fcn_Neuron, activation='relu')(dp1)\n",
    "  # dp2 = Dropout(0.5)(den1)\n",
    "  # den2 = Dense(256, activation='relu')(dp2)\n",
    "  # dp3 = Dropout(0.5)(den2)  \n",
    "  output = Dense(n_outputs, activation='softmax')(fla)\n",
    "  model = Model(inputs = inputs, outputs = output) \n",
    "  return model  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "av8Vgmj5Syei"
   },
   "source": [
    "# 4. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YeziyOZhTnqS"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(trainX, trainy, testX, testy, valX, valy,learningRate,batch_size,num_filter, num_kernel, att_neurons, num_lstm_Neuron, num_fcn_Neuron, n_timesteps, n_steps, n_length,n_features, n_outputs):\n",
    "  model = Sequential()\n",
    "  model = build_model(att_neurons, num_lstm_Neuron, num_fcn_Neuron,num_filter, num_kernel,n_timesteps, n_steps, n_length, n_features, n_outputs)\n",
    "  optimizer = keras.optimizers.Adam(learning_rate=learningRate) \n",
    "  model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\t# earlyStopping= EarlyStopping(monitor='val_loss', patience=50, mode='auto', verbose=1, restore_best_weights=True)\n",
    "  history = model.fit(trainX, trainy, epochs=epochs, validation_data=(valX, valy), batch_size=batch_size, verbose=verbose, shuffle=True)   \n",
    "\t# _, accuracy = model.evaluate(testX, testy,batch_size=batch_size, verbose=0)\n",
    "  model_checkpoint = ModelCheckpoint('CNNLSTMFCN_v1.hdf5', save_best_only=True, monitor='val_accuracy', mode='auto', save_weights_only=True) \n",
    "\t# model_checkpoint= ModelCheckpoint('CNNLSTMFCN_v1.hdf5', save_best_only=True, monitor='val_accuracy', mode='auto', save_weights_only=True) \n",
    "  accuracy = max(history.history['val_accuracy'])\n",
    "  return model, accuracy, history\n",
    "\n",
    "def summarize_results(scores):\n",
    "\tprint(scores)\n",
    "\tm, s = mean(scores), std(scores)\n",
    "\tprint('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n",
    "\n",
    "def run_experiment(repeats, learningRate, batch_size, num_filter, num_kernel, att_neurons, num_lstm_Neuron, num_fcn_Neuron, trainX, trainy, testX, testy, valX, valy,n_timesteps,n_steps,n_length, n_features, n_outputs, cvscores):\n",
    "\t# load data\n",
    "\t# trainX, trainy, testX, testy, valX, valy = load_dataset()\n",
    "\tscores = list()\n",
    "\tfor r in range(repeats):\n",
    "\t\t# model = Sequential()\n",
    "\t\t#build_model()    \n",
    "\t\tmodel, score, history = evaluate_model(trainX, trainy, testX, testy, valX, valy ,learningRate, batch_size,num_filter, num_kernel, att_neurons, num_lstm_Neuron, num_fcn_Neuron,n_timesteps, n_steps, n_length,n_features, n_outputs)\n",
    "\t\tscore = score * 100.0\n",
    "\t\tcvscores.append(score)\n",
    "\t\t# print('>#%d: %.3f' % (r+1, score))\n",
    "\t\t# scores.append(score)\n",
    "\t\tplt.plot(history.history['loss'])\n",
    "\t\tplt.plot(history.history['val_loss'])\n",
    "\t\tplt.title('Training Loss VS Validation Loss')\n",
    "\t\tplt.ylabel('Loss')\n",
    "\t\tplt.xlabel('epoch')\n",
    "\t\tplt.legend(['train', 'val'], loc='upper left')\n",
    "\t\tplt.show()\n",
    "\t\tprint('Training Accuracy : ', max(history.history['accuracy']))   \n",
    "\t\tprint('Validation Accuracy : ', max(history.history['val_accuracy']))  \n",
    "\t\tplt.plot(history.history['accuracy'])\n",
    "\t\tplt.plot(history.history['val_accuracy'])\n",
    "\t\tplt.title('Training ACC VS Validation ACC')\n",
    "\t\tplt.ylabel('Acc')\n",
    "\t\tplt.xlabel('epoch')\n",
    "\t\tplt.legend(['train', 'val'], loc='upper left')\n",
    "\t\tplt.show()  \n",
    "\t\t# if score > 90.0:\n",
    "\t\t#  break\n",
    "\tmodel.summary()  \n",
    "\t# summarize_results(scores)\n",
    "\treturn model, cvscores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZFa0f_LSyej"
   },
   "source": [
    "# 5. Initialize the SPI and TERMIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "IVHHYDwOSyek"
   },
   "outputs": [],
   "source": [
    "def spi_init():\n",
    "        spi.open(0,0)\n",
    "        spi.max_speed_hz =1350000\n",
    "        spi2.open(0,1)\n",
    "        spi2.max_speed_hz =1350000\n",
    "        \n",
    "def termios_init():\n",
    "        newattr[3] = newattr[3] & ~termios.ICANON & ~termios.ECHO\n",
    "        termios.tcsetattr(fd, termios.TCSANOW, newattr)\n",
    "        fcntl.fcntl(fd, fcntl.F_SETFL, oldflags | os.O_NONBLOCK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ggAdczzSyek"
   },
   "source": [
    "# 6. Timer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "tDRBllUDSyel"
   },
   "outputs": [],
   "source": [
    "class RepeatedTimer(object):\n",
    "  def __init__(self, interval, function, *args, **kwargs):\n",
    "    self._timer = None\n",
    "    self.interval = interval\n",
    "    self.function = function\n",
    "    self.args = args\n",
    "    self.kwargs = kwargs\n",
    "    self.is_running = False\n",
    "    self.next_call = time.time()\n",
    "    self.start()\n",
    "\n",
    "  def _run(self):\n",
    "    self.is_running = False\n",
    "    self.start()\n",
    "    self.function(*self.args, **self.kwargs)\n",
    "\n",
    "  def start(self):\n",
    "    if not self.is_running:\n",
    "      self.next_call += self.interval\n",
    "      self._timer = threading.Timer(self.next_call - time.time(), self._run)\n",
    "      self._timer.start()\n",
    "      self.is_running = True\n",
    "\n",
    "  def stop(self):\n",
    "    self._timer.cancel()\n",
    "    self.is_running = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsassCeCSyel"
   },
   "source": [
    "# 7. Read Sensor Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ct_vM5bnSyel"
   },
   "outputs": [],
   "source": [
    "def get_char():\n",
    "    fd = sys.stdin.fileno()\n",
    "    old_settings = termios.tcgetattr(fd)\n",
    "    try:\n",
    "        tty.setraw(sys.stdin.fileno())\n",
    "        ch = sys.stdin.read(1)\n",
    "    finally:\n",
    "        termios.tcsetattr(fd, termios.TCSADRAIN, old_settings)\n",
    "    return ch\n",
    "\n",
    "\n",
    "def analogInput_CE0(channel,spi):\n",
    "    adc = spi.xfer2([1, (8+channel)<<4, 0])\n",
    "    data = ((adc[1]&3) <<8) + adc[2]\n",
    "    return data\n",
    "\n",
    "def analogInput_CE1(channel,spi2):\n",
    "    adc = spi2.xfer2([1, (8+channel)<<4, 0])\n",
    "    data = ((adc[1]&3) <<8) + adc[2]\n",
    "    return data\n",
    "\n",
    "def read_data():\n",
    "        if len(output)  < 500:\n",
    "            aux = []\n",
    "#             aux.append(time.time())\n",
    "            aux.append(analogInput_CE1(2,spi2))\n",
    "            aux.append(analogInput_CE0(3,spi))\n",
    "            aux.append(analogInput_CE0(7,spi))\n",
    "            aux.append(analogInput_CE1(3,spi2))\n",
    "            aux.append(analogInput_CE1(1,spi2))\n",
    "            aux.append(analogInput_CE0(5,spi))\n",
    "            aux.append(analogInput_CE0(1,spi))\n",
    "            aux.append(analogInput_CE0(2,spi))\n",
    "            aux.append(analogInput_CE0(6,spi))\n",
    "            output.append(aux)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZ-XgG8CT6Ps"
   },
   "source": [
    "# load and repshAPE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kyVa_7M6T8nj"
   },
   "outputs": [],
   "source": [
    "trainX, trainy, valX, valy = load_dataset()\n",
    "n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pyx1rSShT_Al"
   },
   "outputs": [],
   "source": [
    "trainX1 = trainX.reshape((trainX.shape[0], n_steps, n_length, n_features)) \n",
    "valX1 = valX.reshape((valX.shape[0],n_steps, n_length, n_features)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QHgLqiWgUBj3"
   },
   "outputs": [],
   "source": [
    "cvscores = []\n",
    "for att_neurons in Attention_Neurons:\n",
    "  for j in range(len(num_fcn_Neurons)):\n",
    "    for num_kernel in num_kernels:\n",
    "      for num_filter in num_filters:  \n",
    "        for learningRate in learning_rate:     \n",
    "          for batch_size in Batch_size:\n",
    "            for i in range(len(num_lstm_Neurons)): \n",
    "              print('n_steps : ', n_steps)\n",
    "              print('n_length : ', n_length)\n",
    "              print('batch_size :  ', batch_size)\n",
    "              print('learningRate :  ', learningRate)\n",
    "              print('num_filter :  ', num_filter)\n",
    "              print('num_kernel :  ', num_kernel)\n",
    "              print('num_fcn_Neurons :  ', num_fcn_Neurons[j])\n",
    "              print('num_lstm_neurons :  ', num_lstm_Neurons[i]) \n",
    "              print('num_Att_neurons : ', att_neurons)          \n",
    "              prev_time = time.time()\n",
    "              model, cvscores = run_experiment(repeats=1, learningRate= learningRate, batch_size = batch_size, num_filter= num_filter, num_kernel = num_kernel, att_neurons = att_neurons, num_lstm_Neuron = num_lstm_Neurons[i], num_fcn_Neuron = num_fcn_Neurons[j],trainX= trainX1, trainy=trainy, testX=None, testy= None, valX=valX1, valy= valy, n_timesteps = n_timesteps, n_steps=n_steps, n_length = n_length, n_features=n_features, n_outputs=n_outputs, cvscores = cvscores)\n",
    "              print('Interval is : ', time.time() - prev_time)\n",
    "              print('-----------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIy2kscbUM1f"
   },
   "source": [
    "# SAVE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gzxEBl9aUO2y"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import save_model, load_model\n",
    "filepath = './CNNLSTMFCN_v3'\n",
    "save_model(model, filepath, save_format='h5')\n",
    "# model = load_model('saved_model4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "UzaCEQMmSyen"
   },
   "outputs": [],
   "source": [
    "from keras.models import save_model, load_model\n",
    "# filepath = './saved_model'\n",
    "# save_model(model, filepath, save_format='h5')\n",
    "model = load_model(\"CNNLSTMFCN_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = [0.001]\n",
    "verbose, epochs  = 0, 20\n",
    "Batch_size = [32]\n",
    "\n",
    "n_steps, n_length = 60, 5\n",
    "num_filters = [32]\n",
    "num_kernels = [3]\n",
    "num_lstm_Neurons = [ 1024]\n",
    "num_heads = [2]\n",
    "num_fcn_Neurons = [1024]\n",
    "Attention_Neurons = [1]\n",
    "\n",
    "model = build_model(num_heads[0], Attention_Neurons[0], num_lstm_Neurons[0], num_fcn_Neurons[0],num_filters[0], num_kernels[0],300, n_steps, n_length, 9, 10)\n",
    "model.load_weights(\"All_asTrain_and_Test_TF20_ACC92.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isCovered(data, digit):\n",
    "    if data[data.digit < 150].count() > 170:\n",
    "        return True\n",
    "    else: \n",
    "        return False\n",
    "\n",
    "def calibration_mode():\n",
    "    print(\"Calibration Mode: Please put your finger tip on the center photodiode  \")\n",
    "    data_cal = pd.DataFrame(output[300:500], columns = ['6', '1', '2', '3', '9', '8', '7', '4', '5'])\n",
    "    max_val = data_cal.max()\n",
    "    print('max_val : ', max_val)\n",
    "    if isCovered(data_cal, 2):\n",
    "        print (\"sensing area is : 3; data_reading area: 1\" )\n",
    "    elif isCovered(data_cal, 1):\n",
    "        print (\"sensing area is : 4; data_reading area: 1\" )     \n",
    "    elif isCovered(data_cal, 3):\n",
    "        print (\"sensing area is : 3; data_reading area: 2\" )\n",
    "    elif isCovered(data_cal, 4) and (not isCovered(data_cal, 5) ):\n",
    "        print (\"sensing area is : 2; data_reading area: 1\" )   \n",
    "    elif isCovered(data_cal, 6) and (not isCovered(data_cal, 5) ):\n",
    "        print (\"sensing area is : 1; data_reading area: 2\" )\n",
    "    elif isCovered(data_cal, 8) and (not isCovered(data_cal, 5) ):\n",
    "        print (\"sensing area is : 1; data_reading area: 3\" )\n",
    "    elif isCovered(data_cal, 7) and (not isCovered(data_cal, 5) ):\n",
    "        print (\"sensing area is : 2; data_reading area: 3\" )\n",
    "    elif isCovered(data_cal, 9) and (not isCovered(data_cal, 5) ):\n",
    "        print (\"sensing area is : 1; data_reading area: 4\" )\n",
    "    else:\n",
    "        print (\"sensing area is : 1; data_reading area: 1\" )   \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "peCE5kmOSyeo",
    "outputId": "91f9ec60-3a35-4b17-d851-dfac8af28c53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n\n",
      "start get input\n",
      "     6    1    2    3    9    8    7    4    5\n",
      "0  259  239  228  358  263  263  269  262  262\n",
      "label is  0\n",
      "n\n",
      "start get input\n",
      "     6    1    2    3    9    8    7    4    5\n",
      "0  246  248  244  344  258  260  260  232  251\n"
     ]
    }
   ],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "from sklearn import preprocessing\n",
    "import cv2\n",
    "\n",
    "def min_max_scale(X, range=(0, 100)):\n",
    "    mi, ma = range\n",
    "    max_list = X.max()\n",
    "    min_list = X.min()\n",
    "    max_val = max_list.max(axis=0)\n",
    "    min_val = min_list.min(axis=0)\n",
    "    X_std = (X - min_val) / (max_val - min_val)\n",
    "    X_scaled = X_std * (ma - mi) + mi\n",
    "    return X_scaled\n",
    "\n",
    "if __name__ == '__main__':  \n",
    "    global spi, spi2, output,fd, oldterm, newattr, oldflags, real_data_3D\n",
    "    spi = spidev.SpiDev()\n",
    "    spi2 = spidev.SpiDev()\n",
    "    output = []\n",
    "    spi_init()\n",
    "    RepeatedTimer(0.01,read_data).start()\n",
    "    \n",
    "    while 1: \n",
    "        if calibration_mode():\n",
    "            break\n",
    "        else:\n",
    "            print(\"Error: ...\")\n",
    "        \n",
    "    while 1:\n",
    "        inkey = input()\n",
    "        if inkey == 'n':\n",
    "            print('start get input')\n",
    "            output = []\n",
    "            sleep(5.5)\n",
    "#             output_3D = []\n",
    "#             output_3D.append(output[0:500])\n",
    "#             real_data_3D = np.array(output_3D)\n",
    "#             print(real_data_3D.shape)\n",
    "            # 1. remove outliers\n",
    "#             print(output.shape)\n",
    "            data_v1 = pd.DataFrame(output[0:500], columns = ['6', '1', '2', '3', '9', '8', '7', '4', '5'])\n",
    "            print(data_v1.head(1))\n",
    "            Q1 = data_v1.quantile(0.01)\n",
    "            Q3 = data_v1.quantile(0.99)\n",
    "            for m in range(1, 10, 1):\n",
    "                data_v1.loc[data_v1[str(m)] > Q3[str(m)], str(m)] = Q3[str(m)]\n",
    "                data_v1.loc[data_v1[str(m)] < Q1[str(m)], str(m)] = Q1[str(m)]\n",
    "            data_v2 = savgol_filter(data_v1[['6', '1', '2', '3', '9', '8', '7', '4', '5']], 35, 7, axis = 0)\n",
    "            data_v2 = pd.DataFrame(data_v2, columns = ['6', '1', '2', '3', '9', '8', '7', '4', '5'])\n",
    "#             print('------------------')\n",
    "#             print(data_v2.head(10))            \n",
    "            max_list = data_v2.max()\n",
    "            max_val = max_list.max(axis = 0)\n",
    "            diff_list = max_val - max_list\n",
    "            for m in range(1, 10, 1):\n",
    "                data_v2.loc[:, str(m)] = data_v2.loc[:, str(m)] + diff_list[str(m)]\n",
    "            data_v3 = min_max_scale(data_v2)\n",
    "#             print('------------------')\n",
    "#             print(data_v3.head(10))\n",
    "            data_v3 = pd.DataFrame(data_v3, columns = ['6', '1', '2', '3', '9', '8', '7', '4', '5'])            \n",
    "            start_point = data_v3[((data_v3['1'] <= 70) & (data_v3['1'] > 0)) | \n",
    "                                  ((data_v3['2'] <= 70) & (data_v3['2'] > 0)) | \n",
    "                                  ((data_v3['3'] <= 70) & (data_v3['3'] > 0)) | \n",
    "                                  ((data_v3['4'] <= 70) & (data_v3['4'] > 0)) | \n",
    "                                  ((data_v3['5'] <= 70) & (data_v3['5'] > 0)) | \n",
    "                                  ((data_v3['6'] <= 70) & (data_v3['6'] > 0)) | \n",
    "                                  ((data_v3['7'] <= 70) & (data_v3['7'] > 0)) | \n",
    "                                  ((data_v3['8'] <= 70) & (data_v3['8'] > 0)) | \n",
    "                                  ((data_v3['9'] <= 70) & (data_v3['9'] > 0)) ]\n",
    "            if (start_point.empty):\n",
    "                break\n",
    "            start = int(start_point.index[0])\n",
    "            end = int(start_point.index[-1])\n",
    "            if (start - 10 >= 0):\n",
    "                start -= 10\n",
    "            if (end + 10 <= 499):\n",
    "                end += 10\n",
    "            pic = cv2.resize(data_v3.loc[start: end, ['6', '1', '2', '3', '9', '8', '7', '4', '5']].values, (9, 300), interpolation =  cv2.INTER_LINEAR)\n",
    "            start_point.iloc[0:0]\n",
    "            data_v4 = pd.DataFrame(pic, columns = ['6', '1', '2', '3', '9', '8', '7', '4', '5'])\n",
    "#             print(data_v4.shape)\n",
    "#             print('------------------')\n",
    "#             print(data_v4.head(10))\n",
    "            data_v4.values.reshape((-1, 300, 9))\n",
    "            real_data_3D = data_v4.values.reshape((-1, 60, 5, 9))\n",
    "#             print(real_data_3D)\n",
    "            label = model.predict(real_data_3D)\n",
    "#             print(\"label is \", label)\n",
    "            print('label is ', np.argmax(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w5OTu8ivSyeq",
    "outputId": "686ec463-9fff-4e1e-fa71-1511ec570975"
   },
   "outputs": [],
   "source": [
    "a = input()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Model_REAL_DATASET.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
