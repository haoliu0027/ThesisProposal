{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQcC3-m8Syea"
   },
   "source": [
    "# 1. Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gcZDMIf4Syee",
    "outputId": "1675f55d-bc7e-4d1c-fe93-c35d74178902"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "print(keras.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_gWmBX9hSyef"
   },
   "outputs": [],
   "source": [
    "\n",
    "import keras\n",
    "from numpy import dstack, hstack\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM, GRU, SimpleRNN, Bidirectional\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TimeDistributed \n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import concatenate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "import time\n",
    "import pandas as pd\n",
    "import csv\n",
    "import spidev\n",
    "from numpy import interp\n",
    "from time import sleep\n",
    "import RPi.GPIO as GPIO\n",
    "import signal\n",
    "import datetime, time\n",
    "# from threading import _Timer\n",
    "import csv\n",
    "import termios,sys,tty, fcntl, os,select\n",
    "import threading\n",
    "from sklearn import preprocessing\n",
    "import cv2\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EEX8kqJESyeg"
   },
   "source": [
    "# 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "14QG521wS5cs"
   },
   "outputs": [],
   "source": [
    "\n",
    "data_filenames = ['Training_data/0/data_0_22_v7.csv','Training_data/0/data_0_22_2_v7.csv','Training_data/0/data_0_121_s1_t1_1_v7.csv', 'Training_data/0/data_0_211_s1_t1_1_v7.csv', \n",
    "                  'Training_data/0/data_0_220_s1_t1_1_v7.csv', 'Training_data/0/data_0_221_s1_t1_1_v7.csv', \n",
    "                  'Training_data/1/data_1_011_021_v7.csv','Training_data/1/data_1_120_122_v7.csv', 'Training_data/1/data_1_021_s1_t1_1_v7.csv',\n",
    "                  'Training_data/1/data_1_120_s1_t1_1_v7.csv','Training_data/1/data_1_122_s1_t1_1_v7.csv', 'Training_data/1/data_1_021_s2_t1_1_v7.csv',\n",
    "                  'Training_data/2/data_2_22_v7.csv','Training_data/2/data_2_22_2_v7.csv','Training_data/2/data_2_121_s1_t1_1_v7.csv','Training_data/2/data_2_211_s1_t1_1_v7.csv',\n",
    "                  'Training_data/2/data_2_220_s1_t1_1_v7.csv','Training_data/2/data_2_221_s1_t1_1_v7.csv','Training_data/2/data_2_222_s1_t1_1_v7.csv',\n",
    "                  'Training_data/3/data_3_22_v7.csv','Training_data/3/data_3_22_2_v7.csv','Training_data/3/data_3_121_s1_t1_1_v7.csv','Training_data/3/data_3_121_s2_t1_1_v7.csv',\n",
    "                  'Training_data/3/data_3_220_s1_t1_1_v7.csv','Training_data/3/data_3_220_s2_t1_1_v7.csv','Training_data/3/data_3_221_s1_t1_1_v7.csv','Training_data/3/data_3_221_s2_t1_1_v7.csv',  \n",
    "                  'Training_data/4/data_4_221_v7.csv', 'Training_data/4/data_4_221_2_v7.csv','Training_data/4/data_4_121_s1_t1_1_v7.csv','Training_data/4/data_4_121_s2_t1_1_v7.csv',\n",
    "                  'Training_data/4/data_4_211_s1_t1_1_v7.csv', 'Training_data/4/data_4_211_s2_t1_1_v7.csv', 'Training_data/4/data_4_210_s1_t1_1_v7.csv','Training_data/4/data_4_210_s2_t1_1_v7.csv',\n",
    "                  'Training_data/4/data_4_220_s1_t1_1_v7.csv','Training_data/4/data_4_220_s2_t1_1_v7.csv','Training_data/4/data_4_221_s1_t1_1_v7.csv','Training_data/4/data_4_221_s2_t1_1_v7.csv',\n",
    "                  'Training_data/5/data_5_221_v7.csv', 'Training_data/5/data_5_221_2_v7.csv', 'Training_data/5/data_5_121_s1_t1_1_v7.csv','Training_data/5/data_5_121_s1_t2_1_v7.csv',\n",
    "                  'Training_data/5/data_5_220_s1_t1_1_v7.csv','Training_data/5/data_5_220_s1_t2_1_v7.csv','Training_data/5/data_5_221_s1_t1_1_v7.csv','Training_data/5/data_5_221_s1_t2_1_v7.csv',\n",
    "                  'Training_data/6/data_6_121_v7.csv', 'Training_data/6/data_6_221_220_v7.csv', 'Training_data/6/data_6_121_s1_t1_1_v7.csv','Training_data/6/data_6_221_s1_t1_1_v7.csv',\n",
    "                  'Training_data/7/data_7_221_v7.csv','Training_data/7/data_7_221_3_v7.csv', 'Training_data/7/data_7_221_2_v7.csv', 'Training_data/7/data_7_121_s1_t1_1_v7.csv',\n",
    "                  'Training_data/7/data_7_121_s2_t1_1_v7.csv','Training_data/7/data_7_121_s3_t1_1_v7.csv', 'Training_data/7/data_7_221_s1_t1_1_v7.csv',\n",
    "                  'Training_data/7/data_7_221_s2_t1_1_v7.csv','Training_data/7/data_7_221_s3_t1_1_v7.csv',   \n",
    "                  'Training_data/8/data_8_221_v7.csv', 'Training_data/8/data_8_221_3_v7.csv', 'Training_data/8/data_8_121_s1_t1_1_v7.csv','Training_data/8/data_8_121_s1_t2_1_v7.csv',\n",
    "                  'Training_data/8/data_8_121_s1_t3_1_v7.csv', 'Training_data/8/data_8_220_s1_t1_1_v7.csv','Training_data/8/data_8_220_s1_t2_1_v7.csv','Training_data/8/data_8_220_s1_t3_1_v7.csv',\n",
    "                  'Training_data/8/data_8_221_s1_t1_1_v7.csv','Training_data/8/data_8_221_s1_t2_1_v7.csv','Training_data/8/data_8_221_s1_t3_1_v7.csv','Training_data/8/data_8_221_s1_t4_1_v7.csv',\n",
    "                  'Training_data/8/data_8_221_s1_t5_1_v7.csv','Training_data/8/data_8_121_s1_t4_1_v7.csv',\n",
    "                  'Training_data/9/data_9_120_v7.csv', 'Training_data/9/data_9_221_220_v7.csv','Training_data/9/data_9_221_220_3_v7.csv',\n",
    "                  'Training_data/9/data_9_120_s1_t1_1_v7.csv','Training_data/9/data_9_121_s1_t1_1_v7.csv','Training_data/9/data_9_220_s1_t1_1_v7.csv','Training_data/9/data_9_221_s1_t1_1_v7.csv']\n",
    "\n",
    "\n",
    "label_filenames = ['Training_data/0/data_0_22_v7.csv_label_v1.txt','Training_data/0/data_0_22_2_v7.csv_label_v1.txt','Training_data/0/data_0_121_s1_t1_1_v7.csv_label_v1.txt', \n",
    "                    'Training_data/0/data_0_211_s1_t1_1_v7.csv_label_v1.txt', 'Training_data/0/data_0_220_s1_t1_1_v7.csv_label_v1.txt', 'Training_data/0/data_0_221_s1_t1_1_v7.csv_label_v1.txt', \n",
    "                  'Training_data/1/data_1_011_021_v7.csv_label_v1.txt','Training_data/1/data_1_120_122_v7.csv_label_v1.txt', 'Training_data/1/data_1_021_s1_t1_1_v7.csv_label_v1.txt',\n",
    "                  'Training_data/1/data_1_120_s1_t1_1_v7.csv_label_v1.txt','Training_data/1/data_1_122_s1_t1_1_v7.csv_label_v1.txt','Training_data/1/data_1_021_s2_t1_1_v7.csv_label_v1.txt',\n",
    "                  'Training_data/2/data_2_22_v7.csv_label_v1.txt','Training_data/2/data_2_22_2_v7.csv_label_v1.txt','Training_data/2/data_2_121_s1_t1_1_v7.csv_label_v1.txt','Training_data/2/data_2_211_s1_t1_1_v7.csv_label_v1.txt',\n",
    "                  'Training_data/2/data_2_220_s1_t1_1_v7.csv_label_v1.txt','Training_data/2/data_2_221_s1_t1_1_v7.csv_label_v1.txt','Training_data/2/data_2_222_s1_t1_1_v7.csv_label_v1.txt',\n",
    "                  'Training_data/3/data_3_22_v7.csv_label_v1.txt','Training_data/3/data_3_22_2_v7.csv_label_v1.txt','Training_data/3/data_3_121_s1_t1_1_v7.csv_label_v1.txt','Training_data/3/data_3_121_s2_t1_1_v7.csv_label_v1.txt',\n",
    "                  'Training_data/3/data_3_220_s1_t1_1_v7.csv_label_v1.txt','Training_data/3/data_3_220_s2_t1_1_v7.csv_label_v1.txt','Training_data/3/data_3_221_s1_t1_1_v7.csv_label_v1.txt','Training_data/3/data_3_221_s2_t1_1_v7.csv_label_v1.txt',  \n",
    "                  'Training_data/4/data_4_221_v7.csv_label_v1.txt', 'Training_data/4/data_4_221_2_v7.csv_label_v1.txt','Training_data/4/data_4_121_s1_t1_1_v7.csv_label_v1.txt','Training_data/4/data_4_121_s2_t1_1_v7.csv_label_v1.txt',\n",
    "                  'Training_data/4/data_4_211_s1_t1_1_v7.csv_label_v1.txt', 'Training_data/4/data_4_211_s2_t1_1_v7.csv_label_v1.txt', 'Training_data/4/data_4_210_s1_t1_1_v7.csv_label_v1.txt','Training_data/4/data_4_210_s2_t1_1_v7.csv_label_v1.txt',\n",
    "                  'Training_data/4/data_4_220_s1_t1_1_v7.csv_label_v1.txt','Training_data/4/data_4_220_s2_t1_1_v7.csv_label_v1.txt','Training_data/4/data_4_221_s1_t1_1_v7.csv_label_v1.txt','Training_data/4/data_4_221_s2_t1_1_v7.csv_label_v1.txt',\n",
    "                  'Training_data/5/data_5_221_v7.csv_label_v1.txt', 'Training_data/5/data_5_221_2_v7.csv_label_v1.txt', 'Training_data/5/data_5_121_s1_t1_1_v7.csv_label_v1.txt','Training_data/5/data_5_121_s1_t2_1_v7.csv_label_v1.txt',\n",
    "                  'Training_data/5/data_5_220_s1_t1_1_v7.csv_label_v1.txt','Training_data/5/data_5_220_s1_t2_1_v7.csv_label_v1.txt','Training_data/5/data_5_221_s1_t1_1_v7.csv_label_v1.txt','Training_data/5/data_5_221_s1_t2_1_v7.csv_label_v1.txt',\n",
    "                  'Training_data/6/data_6_121_v7.csv_label_v1.txt', 'Training_data/6/data_6_221_220_v7.csv_label_v1.txt', 'Training_data/6/data_6_121_s1_t1_1_v7.csv_label_v1.txt','Training_data/6/data_6_221_s1_t1_1_v7.csv_label_v1.txt',\n",
    "                  'Training_data/7/data_7_221_v7.csv_label_v1.txt','Training_data/7/data_7_221_3_v7.csv_label_v1.txt', 'Training_data/7/data_7_221_2_v7.csv_label_v1.txt', 'Training_data/7/data_7_121_s1_t1_1_v7.csv_label_v1.txt',\n",
    "                  'Training_data/7/data_7_121_s2_t1_1_v7.csv_label_v1.txt','Training_data/7/data_7_121_s3_t1_1_v7.csv_label_v1.txt', 'Training_data/7/data_7_221_s1_t1_1_v7.csv_label_v1.txt',\n",
    "                  'Training_data/7/data_7_221_s2_t1_1_v7.csv_label_v1.txt','Training_data/7/data_7_221_s3_t1_1_v7.csv_label_v1.txt',   \n",
    "                  'Training_data/8/data_8_221_v7.csv_label_v1.txt', 'Training_data/8/data_8_221_3_v7.csv_label_v1.txt', 'Training_data/8/data_8_121_s1_t1_1_v7.csv_label_v1.txt','Training_data/8/data_8_121_s1_t2_1_v7.csv_label_v1.txt',\n",
    "                  'Training_data/8/data_8_121_s1_t3_1_v7.csv_label_v1.txt', 'Training_data/8/data_8_220_s1_t1_1_v7.csv_label_v1.txt','Training_data/8/data_8_220_s1_t2_1_v7.csv_label_v1.txt','Training_data/8/data_8_220_s1_t3_1_v7.csv_label_v1.txt',\n",
    "                  'Training_data/8/data_8_221_s1_t1_1_v7.csv_label_v1.txt','Training_data/8/data_8_221_s1_t2_1_v7.csv_label_v1.txt','Training_data/8/data_8_221_s1_t3_1_v7.csv_label_v1.txt',\n",
    "                  'Training_data/8/data_8_221_s1_t4_1_v7.csv_label_v1.txt','Training_data/8/data_8_221_s1_t5_1_v7.csv_label_v1.txt','Training_data/8/data_8_121_s1_t4_1_v7.csv_label_v1.txt',\n",
    "                  'Training_data/9/data_9_120_v7.csv_label_v1.txt', 'Training_data/9/data_9_221_220_v7.csv_label_v1.txt','Training_data/9/data_9_221_220_3_v7.csv_label_v1.txt',\n",
    "                  'Training_data/9/data_9_120_s1_t1_1_v7.csv_label_v1.txt','Training_data/9/data_9_121_s1_t1_1_v7.csv_label_v1.txt','Training_data/9/data_9_220_s1_t1_1_v7.csv_label_v1.txt','Training_data/9/data_9_221_s1_t1_1_v7.csv_label_v1.txt']\n",
    "\n",
    "data_test_filenames = [\n",
    "                        'Testing_data/jundaWu_v2/data_0_jundaWu_v7.csv','Testing_data/jundaWu_v2/data_1_jundaWu_v7.csv','Testing_data/jundaWu_v2/data_2_jundaWu_v7.csv',\n",
    "                        'Testing_data/jundaWu_v2/data_3_jundaWu_v7.csv','Testing_data/jundaWu_v2/data_4_jundaWu_v7.csv','Testing_data/jundaWu_v2/data_5_jundaWu_v7.csv',\n",
    "                        'Testing_data/jundaWu_v2/data_6_jundaWu_v7.csv','Testing_data/jundaWu_v2/data_7_jundaWu_v7.csv','Testing_data/jundaWu_v2/data_8_jundaWu_v7.csv',\n",
    "                        'Testing_data/jundaWu_v2/data_9_jundaWu_v7.csv',\n",
    "                        'Testing_data/shixunWu_v2/data_0_shiXun_v7.csv','Testing_data/shixunWu_v2/data_1_shiXun_v7.csv','Testing_data/shixunWu_v2/data_2_shiXun_v7.csv',\n",
    "                        'Testing_data/shixunWu_v2/data_3_shiXun_v7.csv','Testing_data/shixunWu_v2/data_4_shiXun_v7.csv','Testing_data/shixunWu_v2/data_5_shiXun_v7.csv',\n",
    "                        'Testing_data/shixunWu_v2/data_6_shiXun_v7.csv','Testing_data/shixunWu_v2/data_7_shiXun_v7.csv','Testing_data/shixunWu_v2/data_8_shiXun_v7.csv',\n",
    "                        'Testing_data/shixunWu_v2/data_9_shiXun_v7.csv',\n",
    "                        'Testing_data/xinyunXu/data_0_XinyunXu_v7.csv','Testing_data/xinyunXu/data_1_XinyunXu_v7.csv','Testing_data/xinyunXu/data_2_XinyunXu_v7.csv',\n",
    "                        'Testing_data/xinyunXu/data_3_XinyunXu_v7.csv','Testing_data/xinyunXu/data_4_XinyunXu_v7.csv','Testing_data/xinyunXu/data_5_XinyunXu_v7.csv',\n",
    "                        'Testing_data/xinyunXu/data_6_XinyunXu_v7.csv','Testing_data/xinyunXu/data_7_XinyunXu_v7.csv','Testing_data/xinyunXu/data_8_XinyunXu_v7.csv',\n",
    "                        'Testing_data/xinyunXu/data_9_XinyunXu_v7.csv',\n",
    "                        'Testing_data/yuanhaoXie/data_0_xie_3_v7.csv','Testing_data/yuanhaoXie/data_1_xie_3_v7.csv','Testing_data/yuanhaoXie/data_2_xie_3_v7.csv',\n",
    "                        'Testing_data/yuanhaoXie/data_3_xie_3_v7.csv','Testing_data/yuanhaoXie/data_4_xie_3_v7.csv','Testing_data/yuanhaoXie/data_5_xie_3_v7.csv',\n",
    "                        'Testing_data/yuanhaoXie/data_6_xie_3_v7.csv','Testing_data/yuanhaoXie/data_7_xie_3_v7.csv','Testing_data/yuanhaoXie/data_8_xie_3_v7.csv',\n",
    "                        'Testing_data/yuanhaoXie/data_9_xie_3_v7.csv',\n",
    "                        'Testing_data/yuanhaoXie/data_0_xie_v7.csv','Testing_data/yuanhaoXie/data_1_xie_v7.csv','Testing_data/yuanhaoXie/data_2_xie_v7.csv',\n",
    "                        'Testing_data/yuanhaoXie/data_3_xie_v7.csv','Testing_data/yuanhaoXie/data_4_xie_v7.csv','Testing_data/yuanhaoXie/data_5_xie_v7.csv',\n",
    "                        'Testing_data/yuanhaoXie/data_6_xie_v7.csv','Testing_data/yuanhaoXie/data_7_xie_v7.csv','Testing_data/yuanhaoXie/data_8_xie_v7.csv',\n",
    "                        'Testing_data/yuanhaoXie/data_9_xie_v7.csv',\n",
    "                        'Testing_data/yuanhaoXie/data_0_xie_2_v7.csv', 'Testing_data/yuanhaoXie/data_3_xie_2_v7.csv',\n",
    "                        'Testing_data/zehangWu/data_0_ZehangWu_v7.csv', 'Testing_data/zehangWu/data_1_ZehangWu_v7.csv', 'Testing_data/zehangWu/data_2_ZehangWu_v7.csv',\n",
    "                        'Testing_data/zehangWu/data_3_ZehangWu_v7.csv', 'Testing_data/zehangWu/data_4_ZehangWu_v7.csv', 'Testing_data/zehangWu/data_5_ZehangWu_v7.csv',\n",
    "                        'Testing_data/zehangWu/data_6_ZehangWu_v7.csv', 'Testing_data/zehangWu/data_7_ZehangWu_v7.csv', 'Testing_data/zehangWu/data_8_ZehangWu_v7.csv',\n",
    "                        'Testing_data/zehangWu/data_9_ZehangWu_v7.csv',\n",
    "                        'Testing_data/liang/data_0_Liang_v7.csv','Testing_data/liang/data_1_Liang_v7.csv','Testing_data/liang/data_2_Liang_v7.csv',\n",
    "                        'Testing_data/liang/data_3_Liang_v7.csv','Testing_data/liang/data_4_Liang_v7.csv','Testing_data/liang/data_5_Liang_v7.csv',\n",
    "                        'Testing_data/liang/data_6_Liang_v7.csv','Testing_data/liang/data_7_Liang_v7.csv','Testing_data/liang/data_8_Liang_v7.csv',\n",
    "                        'Testing_data/liang/data_9_Liang_v7.csv',\n",
    "                        'Testing_data/shixunWu/data_0_shiXun_v7.csv', 'Testing_data/shixunWu/data_1_shiXun_v7.csv','Testing_data/shixunWu/data_2_shiXun_v7.csv',\n",
    "                        'Testing_data/shixunWu/data_3_shiXun_v7.csv','Testing_data/shixunWu/data_4_shiXun_v7.csv','Testing_data/shixunWu/data_5_shiXun_v7.csv',\n",
    "                        'Testing_data/shixunWu/data_6_shiXun_v7.csv','Testing_data/shixunWu/data_7_shiXun_v7.csv','Testing_data/shixunWu/data_8_shiXun_v7.csv',\n",
    "                        'Testing_data/shixunWu/data_9_shiXun_v7.csv'\n",
    "\n",
    "                        ]\n",
    "label_test_filenames = [\n",
    "                        'Testing_data/jundaWu_v2/data_0_jundaWu_v7.csv_label_v1.txt','Testing_data/jundaWu_v2/data_1_jundaWu_v7.csv_label_v1.txt','Testing_data/jundaWu_v2/data_2_jundaWu_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/jundaWu_v2/data_3_jundaWu_v7.csv_label_v1.txt','Testing_data/jundaWu_v2/data_4_jundaWu_v7.csv_label_v1.txt','Testing_data/jundaWu_v2/data_5_jundaWu_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/jundaWu_v2/data_6_jundaWu_v7.csv_label_v1.txt','Testing_data/jundaWu_v2/data_7_jundaWu_v7.csv_label_v1.txt','Testing_data/jundaWu_v2/data_8_jundaWu_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/jundaWu_v2/data_9_jundaWu_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/shixunWu_v2/data_0_shiXun_v7.csv_label_v1.txt','Testing_data/shixunWu_v2/data_1_shiXun_v7.csv_label_v1.txt','Testing_data/shixunWu_v2/data_2_shiXun_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/shixunWu_v2/data_3_shiXun_v7.csv_label_v1.txt','Testing_data/shixunWu_v2/data_4_shiXun_v7.csv_label_v1.txt','Testing_data/shixunWu_v2/data_5_shiXun_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/shixunWu_v2/data_6_shiXun_v7.csv_label_v1.txt','Testing_data/shixunWu_v2/data_7_shiXun_v7.csv_label_v1.txt','Testing_data/shixunWu_v2/data_8_shiXun_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/shixunWu_v2/data_9_shiXun_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/xinyunXu/data_0_XinyunXu_v7.csv_label_v1.txt','Testing_data/xinyunXu/data_1_XinyunXu_v7.csv_label_v1.txt','Testing_data/xinyunXu/data_2_XinyunXu_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/xinyunXu/data_3_XinyunXu_v7.csv_label_v1.txt','Testing_data/xinyunXu/data_4_XinyunXu_v7.csv_label_v1.txt','Testing_data/xinyunXu/data_5_XinyunXu_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/xinyunXu/data_6_XinyunXu_v7.csv_label_v1.txt','Testing_data/xinyunXu/data_7_XinyunXu_v7.csv_label_v1.txt','Testing_data/xinyunXu/data_8_XinyunXu_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/xinyunXu/data_9_XinyunXu_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/yuanhaoXie/data_0_xie_3_v7.csv_label_v1.txt','Testing_data/yuanhaoXie/data_1_xie_3_v7.csv_label_v1.txt','Testing_data/yuanhaoXie/data_2_xie_3_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/yuanhaoXie/data_3_xie_3_v7.csv_label_v1.txt','Testing_data/yuanhaoXie/data_4_xie_3_v7.csv_label_v1.txt','Testing_data/yuanhaoXie/data_5_xie_3_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/yuanhaoXie/data_6_xie_3_v7.csv_label_v1.txt','Testing_data/yuanhaoXie/data_7_xie_3_v7.csv_label_v1.txt','Testing_data/yuanhaoXie/data_8_xie_3_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/yuanhaoXie/data_9_xie_3_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/yuanhaoXie/data_0_xie_v7.csv_label_v1.txt','Testing_data/yuanhaoXie/data_1_xie_v7.csv_label_v1.txt','Testing_data/yuanhaoXie/data_2_xie_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/yuanhaoXie/data_3_xie_v7.csv_label_v1.txt','Testing_data/yuanhaoXie/data_4_xie_v7.csv_label_v1.txt','Testing_data/yuanhaoXie/data_5_xie_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/yuanhaoXie/data_6_xie_v7.csv_label_v1.txt','Testing_data/yuanhaoXie/data_7_xie_v7.csv_label_v1.txt','Testing_data/yuanhaoXie/data_8_xie_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/yuanhaoXie/data_9_xie_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/yuanhaoXie/data_0_xie_2_v7.csv_label_v1.txt', 'Testing_data/yuanhaoXie/data_3_xie_2_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/zehangWu/data_0_ZehangWu_v7.csv_label_v1.txt', 'Testing_data/zehangWu/data_1_ZehangWu_v7.csv_label_v1.txt', 'Testing_data/zehangWu/data_2_ZehangWu_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/zehangWu/data_3_ZehangWu_v7.csv_label_v1.txt', 'Testing_data/zehangWu/data_4_ZehangWu_v7.csv_label_v1.txt', 'Testing_data/zehangWu/data_5_ZehangWu_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/zehangWu/data_6_ZehangWu_v7.csv_label_v1.txt', 'Testing_data/zehangWu/data_7_ZehangWu_v7.csv_label_v1.txt', 'Testing_data/zehangWu/data_8_ZehangWu_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/zehangWu/data_9_ZehangWu_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/liang/data_0_Liang_v7.csv_label_v1.txt','Testing_data/liang/data_1_Liang_v7.csv_label_v1.txt','Testing_data/liang/data_2_Liang_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/liang/data_3_Liang_v7.csv_label_v1.txt','Testing_data/liang/data_4_Liang_v7.csv_label_v1.txt','Testing_data/liang/data_5_Liang_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/liang/data_6_Liang_v7.csv_label_v1.txt','Testing_data/liang/data_7_Liang_v7.csv_label_v1.txt','Testing_data/liang/data_8_Liang_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/liang/data_9_Liang_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/shixunWu/data_0_shiXun_v7.csv_label_v1.txt', 'Testing_data/shixunWu/data_1_shiXun_v7.csv_label_v1.txt','Testing_data/shixunWu/data_2_shiXun_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/shixunWu/data_3_shiXun_v7.csv_label_v1.txt','Testing_data/shixunWu/data_4_shiXun_v7.csv_label_v1.txt','Testing_data/shixunWu/data_5_shiXun_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/shixunWu/data_6_shiXun_v7.csv_label_v1.txt','Testing_data/shixunWu/data_7_shiXun_v7.csv_label_v1.txt','Testing_data/shixunWu/data_8_shiXun_v7.csv_label_v1.txt',\n",
    "                        'Testing_data/shixunWu/data_9_shiXun_v7.csv_label_v1.txt'\n",
    "\n",
    "                        ]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KAB_xh0JTJJU"
   },
   "outputs": [],
   "source": [
    "def create_label_frame(file_name):\n",
    "  label_frame = pd.DataFrame(columns = ['label'])\n",
    "  for i, file in enumerate(file_name):\n",
    "    data = pd.read_csv(file, names = ['label'])\n",
    "    label_frame = pd.concat([label_frame, data], axis = 0, ignore_index = True)\n",
    "  return label_frame\n",
    "\n",
    "def create_data_frame(file_name):\n",
    "  data_frame = []\n",
    "  for i, file in enumerate(file_name):\n",
    "    data = pd.read_csv(file)\n",
    "    data_frame.append (data)\n",
    "  data_frame = pd.concat(data_frame, join = 'outer')\n",
    "  return data_frame\n",
    "\n",
    "def data_3D_transfer(photodiode_arr, data_frame):\n",
    "  data_3D = list()\n",
    "  for i, photodiode_name in enumerate(photodiode_arr):\n",
    "    data_3D.append(data_frame[photodiode_name].values.reshape(-1, ))  \n",
    "  data_3D = dstack(data_3D)\n",
    "  data_3D = data_3D.reshape((-1, 300, 9))\n",
    "  return data_3D\n",
    "\n",
    "def load_dataset():\n",
    "                                         \n",
    "  photodiode_arr = ['6', '1', '2', '3', '9', '8', '7', '4', '5']\n",
    "  label_frame = create_label_frame(label_filenames)\n",
    "  label_test_frame = create_label_frame(label_test_filenames)\n",
    "  data_frame = create_data_frame(data_filenames)\n",
    "  data_test_frame = create_data_frame(data_test_filenames)\n",
    "  data_3D = data_3D_transfer(photodiode_arr, data_frame)\n",
    "  data_test_3D = data_3D_transfer(photodiode_arr, data_test_frame)\n",
    "  data_y = label_frame\n",
    "  data_test_y = label_test_frame\n",
    "  data_y = to_categorical(data_y,num_classes=10)\n",
    "  print('data_y : ', len(data_y))\n",
    "  print('data_test_y : ', len(data_test_y))\n",
    "  data_test_y = to_categorical(data_test_y,num_classes=10)\n",
    "  # Xtra,Xval,Ytra,Yval = train_test_split(data_3D,data_y,test_size=0.2,shuffle=True)\n",
    "  return  data_3D, data_y, data_test_3D,  data_test_y\n",
    "  # return  Xtra,Ytra, data_test_3D,  data_test_y, Xval,Yval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3QoyTP_STRnH"
   },
   "source": [
    "# not use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SwgcOHEjSyeg"
   },
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "  # data_filenames = ['data_0v3_2.csv','data_1v3_2.csv','data_2v3_2.csv','data_3v3_2.csv','data_7v3_2.csv','data_8v3_2.csv',\n",
    "                    # 'data_9v3_2.csv','data_4v3_3.csv','data_5v3_3.csv','data_6v3_3.csv']\n",
    "  data_filenames = ['data_0_v5_3.csv','data_1_v5_3.csv','data_2_v5_3.csv','data_3_v5_3.csv','data_4_v5_3.csv','data_5_v5_3.csv','data_6_v5_3.csv','data_7_v5_3.csv'\n",
    "  ,'data_8_v5_3.csv','data_9_v5_3.csv']\n",
    "  photodiode_arr = ['6','1','2','3','9','8','7','4','5']\n",
    "  data_frame = []\n",
    "  for i,file in enumerate (data_filenames):\n",
    "    data = pd.read_csv(file)\n",
    "    data_frame.append (data)\n",
    "  data_frame = pd.concat(data_frame, join = 'inner')\n",
    "  data_3D = list()\n",
    "  for i, photodiode_name in enumerate(photodiode_arr):\n",
    "    data_3D.append(data_frame[photodiode_name].values.reshape(-1, ))  \n",
    "  data_3D = dstack(data_3D)\n",
    "  data_3D = data_3D.reshape((-1, 500,9))\n",
    "  data_y = pd.read_csv('label_v5.csv')\n",
    "  result_none = np.where(data_y == -1)\n",
    "  data_y = data_y.drop([data_y.index[260],   data_y.index[891],   data_y.index[892]], axis = 0)\n",
    "  data_3D = np.delete(data_3D, [260, 891, 892 ], axis = 0)\n",
    "  data_y = to_categorical(data_y,num_classes=10)\n",
    "  Xtra,Xval,Ytra,Yval = train_test_split(data_3D,data_y,test_size=0.2,shuffle=True)\n",
    "  return Xtra, Ytra, Xval, Yval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71agi6z2Syeh"
   },
   "source": [
    "# 3. Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xiUFWeJGTlYw"
   },
   "outputs": [],
   "source": [
    "learning_rate = [0.001]\n",
    "verbose, epochs  = 0, 20\n",
    "Batch_size = [32]\n",
    "\n",
    "n_steps, n_length = 60, 5\n",
    "num_filters = [32]\n",
    "num_kernels = [3]\n",
    "num_lstm_Neurons = [1024]\n",
    "num_fcn_Neurons = [1024]\n",
    "Attention_Neurons = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3TjcZBGYTVGc"
   },
   "outputs": [],
   "source": [
    "def build_model(att_neuron, num_lstm_Neuron, num_fcn_Neuron, num_filter,num_kernel, n_timesteps, n_steps, n_length, n_features, n_outputs):\n",
    "  inputs = Input(shape = (n_steps, n_length, n_features))\n",
    "  TDCnn = TimeDistributed(Conv1D(filters= 32, kernel_size=3,  activation = 'relu'))(inputs)\n",
    "  dp = TimeDistributed(Dropout(0.5))(TDCnn)\n",
    "  Pool = TimeDistributed(MaxPooling1D(pool_size =3))(dp)\n",
    "  flat = TimeDistributed(Flatten())(Pool)\n",
    "  gru = GRU(num_lstm_Neuron, return_sequences = False)(flat)\n",
    "  dp1 = Dropout(0.5)(gru)\n",
    "  den1 = Dense(num_fcn_Neuron, activation='relu')(dp1)\n",
    "  dp2 = Dropout(0.5)(den1)\n",
    "  den2 = Dense(num_fcn_Neuron, activation='relu')(dp2)\n",
    "  dp3 = Dropout(0.5)(den2)\n",
    "  output = Dense(n_outputs, activation='softmax')(dp3)\n",
    "  model = Model(inputs = inputs, outputs = output) \n",
    "  return model   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_OJzSBoTdhG"
   },
   "source": [
    "# not use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fNVuebD4Syeh"
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_model(): \n",
    "  # model.add(GRU(150, input_shape=(n_timesteps,n_features), return_sequences=True))\n",
    "  # model.add(GRU(50))\n",
    "  model.add(GRU(80, input_shape=(n_timesteps,n_features), return_sequences=False))\n",
    "  # model.add(LSTM(80))\n",
    "  # model.add(Dropout(0.2))\n",
    "  # model.add(GRU(150))\n",
    "  # model.add(Bidirectional( GRU(100))) \n",
    "  # model.add(Dropout(0.5))\n",
    "  # model.add(Dense(100, activation='relu'))\n",
    "  model.add(Dense(80, activation='relu'))\n",
    "  model.add(Dense(n_outputs, activation='softmax'))\n",
    "  model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "av8Vgmj5Syei"
   },
   "source": [
    "# 4. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YeziyOZhTnqS"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(trainX, trainy, testX, testy, valX, valy,learningRate,batch_size,num_filter, num_kernel, att_neurons, num_lstm_Neuron, num_fcn_Neuron, n_timesteps, n_steps, n_length,n_features, n_outputs):\n",
    "  model = Sequential()\n",
    "  model = build_model(att_neurons, num_lstm_Neuron, num_fcn_Neuron,num_filter, num_kernel,n_timesteps, n_steps, n_length, n_features, n_outputs)\n",
    "  optimizer = keras.optimizers.Adam(learning_rate=learningRate) \n",
    "  model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\t# earlyStopping= EarlyStopping(monitor='val_loss', patience=50, mode='auto', verbose=1, restore_best_weights=True)\n",
    "  history = model.fit(trainX, trainy, epochs=epochs, validation_data=(valX, valy), batch_size=batch_size, verbose=verbose, shuffle=True)   \n",
    "\t# _, accuracy = model.evaluate(testX, testy,batch_size=batch_size, verbose=0)\n",
    "  model_checkpoint = ModelCheckpoint('CNNLSTMFCN_v1.hdf5', save_best_only=True, monitor='val_accuracy', mode='auto', save_weights_only=True) \n",
    "\t# model_checkpoint= ModelCheckpoint('CNNLSTMFCN_v1.hdf5', save_best_only=True, monitor='val_accuracy', mode='auto', save_weights_only=True) \n",
    "  accuracy = max(history.history['val_accuracy'])\n",
    "  return model, accuracy, history\n",
    "\n",
    "def summarize_results(scores):\n",
    "\tprint(scores)\n",
    "\tm, s = mean(scores), std(scores)\n",
    "\tprint('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n",
    "\n",
    "def run_experiment(repeats, learningRate, batch_size, num_filter, num_kernel, att_neurons, num_lstm_Neuron, num_fcn_Neuron, trainX, trainy, testX, testy, valX, valy,n_timesteps,n_steps,n_length, n_features, n_outputs, cvscores):\n",
    "\t# load data\n",
    "\t# trainX, trainy, testX, testy, valX, valy = load_dataset()\n",
    "\tscores = list()\n",
    "\tfor r in range(repeats):\n",
    "\t\t# model = Sequential()\n",
    "\t\t#build_model()    \n",
    "\t\tmodel, score, history = evaluate_model(trainX, trainy, testX, testy, valX, valy ,learningRate, batch_size,num_filter, num_kernel, att_neurons, num_lstm_Neuron, num_fcn_Neuron,n_timesteps, n_steps, n_length,n_features, n_outputs)\n",
    "\t\tscore = score * 100.0\n",
    "\t\tcvscores.append(score)\n",
    "\t\t# print('>#%d: %.3f' % (r+1, score))\n",
    "\t\t# scores.append(score)\n",
    "\t\tplt.plot(history.history['loss'])\n",
    "\t\tplt.plot(history.history['val_loss'])\n",
    "\t\tplt.title('Training Loss VS Validation Loss')\n",
    "\t\tplt.ylabel('Loss')\n",
    "\t\tplt.xlabel('epoch')\n",
    "\t\tplt.legend(['train', 'val'], loc='upper left')\n",
    "\t\tplt.show()\n",
    "\t\tprint('Training Accuracy : ', max(history.history['accuracy']))   \n",
    "\t\tprint('Validation Accuracy : ', max(history.history['val_accuracy']))  \n",
    "\t\tplt.plot(history.history['accuracy'])\n",
    "\t\tplt.plot(history.history['val_accuracy'])\n",
    "\t\tplt.title('Training ACC VS Validation ACC')\n",
    "\t\tplt.ylabel('Acc')\n",
    "\t\tplt.xlabel('epoch')\n",
    "\t\tplt.legend(['train', 'val'], loc='upper left')\n",
    "\t\tplt.show()  \n",
    "\t\t# if score > 90.0:\n",
    "\t\t#  break\n",
    "\tmodel.summary()  \n",
    "\t# summarize_results(scores)\n",
    "\treturn model, cvscores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPLl5WORTsNG"
   },
   "source": [
    "# not use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3_yDBy73Syei"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        model_checkpoint= ModelCheckpoint('RNN_model_weights.hdf5', save_best_only=True, monitor='val_loss', \n",
    "                                      mode='auto', save_weights_only=True)\n",
    "        # fit network\n",
    "        history = model.fit(trainX, trainy, epochs=epochs, validation_split=0.2, batch_size=batch_size, verbose=verbose)\n",
    "        # evaluate model\n",
    "        _, accuracy = model.evaluate(testX, testy,batch_size=batch_size, verbose=0)\n",
    "        return accuracy, history\n",
    "\n",
    "def summarize_results(scores):\n",
    "        print(scores)\n",
    "        m, s = mean(scores), std(scores)\n",
    "        print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n",
    "\n",
    "def run_experiment(repeats=10):\n",
    "   \n",
    "        build_model()\n",
    "        \n",
    "        scores = list()\n",
    "        for r in range(repeats):\n",
    "            score, history = evaluate_model(trainX, trainy, testX, testy)\n",
    "            score = score * 100.0\n",
    "            print('>#%d: %.3f' % (r+1, score))\n",
    "            scores.append(score)\n",
    "#             plt.plot(history.history['loss'])\n",
    "#             plt.plot(history.history['val_loss'])\n",
    "#             plt.title('Training Loss VS Validation Loss')\n",
    "#             plt.ylabel('Loss')\n",
    "#             plt.xlabel('epoch')\n",
    "#             plt.legend(['train', 'val'], loc='upper left')\n",
    "#             plt.show()\n",
    "        # summarize results\n",
    "        summarize_results(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZFa0f_LSyej"
   },
   "source": [
    "# 5. Initialize the SPI and TERMIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "IVHHYDwOSyek"
   },
   "outputs": [],
   "source": [
    "def spi_init():\n",
    "        spi.open(0,0)\n",
    "        spi.max_speed_hz =1350000\n",
    "        spi2.open(0,1)\n",
    "        spi2.max_speed_hz =1350000\n",
    "        \n",
    "def termios_init():\n",
    "        newattr[3] = newattr[3] & ~termios.ICANON & ~termios.ECHO\n",
    "        termios.tcsetattr(fd, termios.TCSANOW, newattr)\n",
    "        fcntl.fcntl(fd, fcntl.F_SETFL, oldflags | os.O_NONBLOCK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ggAdczzSyek"
   },
   "source": [
    "# 6. Timer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tDRBllUDSyel"
   },
   "outputs": [],
   "source": [
    "class RepeatedTimer(object):\n",
    "  def __init__(self, interval, function, *args, **kwargs):\n",
    "    self._timer = None\n",
    "    self.interval = interval\n",
    "    self.function = function\n",
    "    self.args = args\n",
    "    self.kwargs = kwargs\n",
    "    self.is_running = False\n",
    "    self.next_call = time.time()\n",
    "    self.start()\n",
    "\n",
    "  def _run(self):\n",
    "    self.is_running = False\n",
    "    self.start()\n",
    "    self.function(*self.args, **self.kwargs)\n",
    "\n",
    "  def start(self):\n",
    "    if not self.is_running:\n",
    "      self.next_call += self.interval\n",
    "      self._timer = threading.Timer(self.next_call - time.time(), self._run)\n",
    "      self._timer.start()\n",
    "      self.is_running = True\n",
    "\n",
    "  def stop(self):\n",
    "    self._timer.cancel()\n",
    "    self.is_running = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsassCeCSyel"
   },
   "source": [
    "# 7. Read Sensor Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ct_vM5bnSyel"
   },
   "outputs": [],
   "source": [
    "def get_char():\n",
    "    fd = sys.stdin.fileno()\n",
    "    old_settings = termios.tcgetattr(fd)\n",
    "    try:\n",
    "        tty.setraw(sys.stdin.fileno())\n",
    "        ch = sys.stdin.read(1)\n",
    "    finally:\n",
    "        termios.tcsetattr(fd, termios.TCSADRAIN, old_settings)\n",
    "    return ch\n",
    "\n",
    "\n",
    "def analogInput_CE0(channel,spi):\n",
    "    adc = spi.xfer2([1, (8+channel)<<4, 0])\n",
    "    data = ((adc[1]&3) <<8) + adc[2]\n",
    "    return data\n",
    "\n",
    "def analogInput_CE1(channel,spi2):\n",
    "    adc = spi2.xfer2([1, (8+channel)<<4, 0])\n",
    "    data = ((adc[1]&3) <<8) + adc[2]\n",
    "    return data\n",
    "\n",
    "def read_data():\n",
    "        if len(output)  < 500:\n",
    "            aux = []\n",
    "#             aux.append(time.time())\n",
    "            aux.append(analogInput_CE1(2,spi2))\n",
    "            aux.append(analogInput_CE0(3,spi))\n",
    "            aux.append(analogInput_CE0(7,spi))\n",
    "            aux.append(analogInput_CE1(3,spi2))\n",
    "            aux.append(analogInput_CE1(1,spi2))\n",
    "            aux.append(analogInput_CE0(5,spi))\n",
    "            aux.append(analogInput_CE0(1,spi))\n",
    "            aux.append(analogInput_CE0(2,spi))\n",
    "            aux.append(analogInput_CE0(6,spi))\n",
    "            output.append(aux)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZ-XgG8CT6Ps"
   },
   "source": [
    "# load and repshAPE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kyVa_7M6T8nj"
   },
   "outputs": [],
   "source": [
    "trainX, trainy, valX, valy = load_dataset()\n",
    "n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pyx1rSShT_Al"
   },
   "outputs": [],
   "source": [
    "trainX1 = trainX.reshape((trainX.shape[0], n_steps, n_length, n_features)) \n",
    "valX1 = valX.reshape((valX.shape[0],n_steps, n_length, n_features)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QHgLqiWgUBj3"
   },
   "outputs": [],
   "source": [
    "cvscores = []\n",
    "for att_neurons in Attention_Neurons:\n",
    "  for j in range(len(num_fcn_Neurons)):\n",
    "    for num_kernel in num_kernels:\n",
    "      for num_filter in num_filters:  \n",
    "        for learningRate in learning_rate:     \n",
    "          for batch_size in Batch_size:\n",
    "            for i in range(len(num_lstm_Neurons)): \n",
    "              print('n_steps : ', n_steps)\n",
    "              print('n_length : ', n_length)\n",
    "              print('batch_size :  ', batch_size)\n",
    "              print('learningRate :  ', learningRate)\n",
    "              print('num_filter :  ', num_filter)\n",
    "              print('num_kernel :  ', num_kernel)\n",
    "              print('num_fcn_Neurons :  ', num_fcn_Neurons[j])\n",
    "              print('num_lstm_neurons :  ', num_lstm_Neurons[i]) \n",
    "              print('num_Att_neurons : ', att_neurons)          \n",
    "              prev_time = time.time()\n",
    "              model, cvscores = run_experiment(repeats=1, learningRate= learningRate, batch_size = batch_size, num_filter= num_filter, num_kernel = num_kernel, att_neurons = att_neurons, num_lstm_Neuron = num_lstm_Neurons[i], num_fcn_Neuron = num_fcn_Neurons[j],trainX= trainX1, trainy=trainy, testX=None, testy= None, valX=valX1, valy= valy, n_timesteps = n_timesteps, n_steps=n_steps, n_length = n_length, n_features=n_features, n_outputs=n_outputs, cvscores = cvscores)\n",
    "              print('Interval is : ', time.time() - prev_time)\n",
    "              print('-----------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIy2kscbUM1f"
   },
   "source": [
    "# SAVE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gzxEBl9aUO2y"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import save_model, load_model\n",
    "filepath = './CNNLSTMFCN_v3'\n",
    "save_model(model, filepath, save_format='h5')\n",
    "# model = load_model('saved_model4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXnMS1pLT1gt"
   },
   "source": [
    "# not use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PY1425_USyem",
    "outputId": "bbb00556-f4b6-42a4-d169-b2f44ad08191"
   },
   "outputs": [],
   "source": [
    "    # Training model\n",
    "prev_time = time.time()\n",
    "verbose, epochs, batch_size, num_Neurons = 0, 40, 128, 100\n",
    "trainX, trainy, testX, testy = load_dataset()\n",
    "n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]    \n",
    "model = Sequential() \n",
    "run_experiment()\n",
    "print('Interval is : ', time.time() - prev_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "UzaCEQMmSyen"
   },
   "outputs": [],
   "source": [
    "from keras.models import save_model, load_model\n",
    "# filepath = './saved_model'\n",
    "# save_model(model, filepath, save_format='h5')\n",
    "model = load_model(\"CNNLSTMFCN_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LyGKq9rHSyen",
    "outputId": "934a46d1-15db-4bff-b940-e9e68c503259"
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    \n",
    "    \n",
    "    global spi, spi2, output,fd, oldterm, newattr, oldflags, real_data_3D\n",
    "    spi = spidev.SpiDev()\n",
    "    spi2 = spidev.SpiDev()\n",
    "    output = []\n",
    "#     fd = sys.stdin.fileno()\n",
    "#     oldterm = termios.tcgetattr(fd)\n",
    "#     oldflags = fcntl.fcntl(fd, fcntl.F_GETFL)\n",
    "#     newattr = termios.tcgetattr(fd)\n",
    "    min_max_scaler = preprocessing.MinMaxScaler(feature_range = (0, 100), copy = True)\n",
    "    spi_init()\n",
    "#     termios_init()\n",
    "    RepeatedTimer(0.01,read_data).start()\n",
    "    while 1:\n",
    "        inkey = input()\n",
    "        if inkey == 'n':\n",
    "            print('start get input')\n",
    "            output = []\n",
    "            sleep(5.5)\n",
    "            output_3D = []\n",
    "            output_3D.append(min_max_scaler.fit_transform(output[0:500]))\n",
    "            real_data_3D = np.array(output_3D)\n",
    "#             real_data_3D = min_max_scaler.fit_transform()\n",
    "#             print(real_data_3D.shape)\n",
    "            real_data_3D.reshape((-1, 300, 9))\n",
    "            real_data_3D = real_data_3D.reshape((-1, 60, 5, 9))\n",
    "#             print(real_data_3D)\n",
    "            label = model.predict(real_data_3D)\n",
    "#             print(\"label is \", label)\n",
    "            print('label is ', np.argmax(label))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "peCE5kmOSyeo",
    "outputId": "91f9ec60-3a35-4b17-d851-dfac8af28c53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n\n",
      "start get input\n",
      "     6    1    2    3    9    8    7    4    5\n",
      "0  218  263  237  207  246  213  259  285  205\n",
      "label is  0\n",
      "n\n",
      "start get input\n",
      "     6    1    2    3    9    8    7    4    5\n",
      "0  214  236  175  288  211  301  248  207  280\n",
      "label is  0\n",
      "n\n",
      "start get input\n",
      "     6    1    2    3    9    8    7    4    5\n",
      "0  245  233  183  235  278  216  230  225  242\n",
      "label is  7\n",
      "n\n",
      "start get input\n",
      "     6    1    2    3    9    8    7    4    5\n",
      "0  277  213  168  264  291  264  217  260  277\n",
      "label is  7\n",
      "n\n",
      "start get input\n",
      "     6    1    2    3    9    8    7    4    5\n",
      "0  210  272  203  298  254  242  245  204  285\n",
      "label is  2\n",
      "n\n",
      "start get input\n",
      "     6    1    2    3    9    8    7    4    5\n",
      "0  231  207  266  269  231  230  249  285  270\n",
      "label is  3\n",
      "n\n",
      "start get input\n",
      "     6    1    2    3    9    8    7    4    5\n",
      "0  224  172  201  217  191  202  205  217  235\n",
      "label is  2\n",
      "n\n",
      "start get input\n",
      "     6    1    2    3    9    8    7    4    5\n",
      "0  230  146  148  257  244  259  184  201  193\n",
      "label is  2\n",
      "n\n",
      "start get input\n",
      "     6    1    2    3    9    8    7    4    5\n",
      "0  199  187  237  265  216  240  263  212  212\n",
      "label is  5\n",
      "n\n",
      "start get input\n",
      "     6    1    2    3    9    8    7    4    5\n",
      "0  248  158  187  231  298  222  191  227  281\n",
      "label is  2\n",
      "n\n",
      "start get input\n",
      "     6    1    2    3    9    8    7    4    5\n",
      "0  197  225  224  274  244  249  251  264  254\n",
      "label is  2\n",
      "n\n",
      "start get input\n",
      "     6    1    2    3    9    8    7    4    5\n",
      "0  193  178  145  211  215  271  230  211  212\n",
      "label is  1\n",
      "n\n",
      "start get input\n",
      "     6    1    2    3    9    8    7    4    5\n",
      "0  196  186  243  296  194  289  210  253  250\n",
      "label is  1\n",
      "n\n",
      "start get input\n",
      "     6    1    2    3    9    8    7    4    5\n",
      "0  264  268  156  270  275  220  229  236  213\n",
      "label is  9\n",
      "n\n",
      "start get input\n",
      "     6    1    2    3    9    8    7    4    5\n",
      "0  235  272  271  260  281  213  218  261  276\n",
      "label is  2\n"
     ]
    }
   ],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "from sklearn import preprocessing\n",
    "import cv2\n",
    "\n",
    "def min_max_scale(X, range=(0, 100)):\n",
    "    mi, ma = range\n",
    "    max_list = X.max()\n",
    "    min_list = X.min()\n",
    "    max_val = max_list.max(axis=0)\n",
    "    min_val = min_list.min(axis=0)\n",
    "    X_std = (X - min_val) / (max_val - min_val)\n",
    "    X_scaled = X_std * (ma - mi) + mi\n",
    "    return X_scaled\n",
    "\n",
    "if __name__ == '__main__':  \n",
    "    global spi, spi2, output,fd, oldterm, newattr, oldflags, real_data_3D\n",
    "    spi = spidev.SpiDev()\n",
    "    spi2 = spidev.SpiDev()\n",
    "    output = []\n",
    "    spi_init()\n",
    "    RepeatedTimer(0.01,read_data).start()\n",
    "    while 1:\n",
    "        inkey = input()\n",
    "        if inkey == 'n':\n",
    "            print('start get input')\n",
    "            output = []\n",
    "            sleep(5.5)\n",
    "#             output_3D = []\n",
    "#             output_3D.append(output[0:500])\n",
    "#             real_data_3D = np.array(output_3D)\n",
    "#             print(real_data_3D.shape)\n",
    "            # 1. remove outliers\n",
    "#             print(output.shape)\n",
    "            data_v1 = pd.DataFrame(output[0:500], columns = ['6', '1', '2', '3', '9', '8', '7', '4', '5'])\n",
    "            print(data_v1.head(1))\n",
    "            Q1 = data_v1.quantile(0.01)\n",
    "            Q3 = data_v1.quantile(0.99)\n",
    "            for m in range(1, 10, 1):\n",
    "                data_v1.loc[data_v1[str(m)] > Q3[str(m)], str(m)] = Q3[str(m)]\n",
    "                data_v1.loc[data_v1[str(m)] < Q1[str(m)], str(m)] = Q1[str(m)]\n",
    "            data_v2 = savgol_filter(data_v1[['6', '1', '2', '3', '9', '8', '7', '4', '5']], 35, 7, axis = 0)\n",
    "            data_v2 = pd.DataFrame(data_v2, columns = ['6', '1', '2', '3', '9', '8', '7', '4', '5'])\n",
    "#             print('------------------')\n",
    "#             print(data_v2.head(10))            \n",
    "            max_list = data_v2.max()\n",
    "            max_val = max_list.max(axis = 0)\n",
    "            diff_list = max_val - max_list\n",
    "            for m in range(1, 10, 1):\n",
    "                data_v2.loc[:, str(m)] = data_v2.loc[:, str(m)] + diff_list[str(m)]\n",
    "            data_v3 = min_max_scale(data_v2)\n",
    "#             print('------------------')\n",
    "#             print(data_v3.head(10))\n",
    "            data_v3 = pd.DataFrame(data_v3, columns = ['6', '1', '2', '3', '9', '8', '7', '4', '5'])            \n",
    "            start_point = data_v3[((data_v3['1'] <= 70) & (data_v3['1'] > 0)) | \n",
    "                                  ((data_v3['2'] <= 70) & (data_v3['2'] > 0)) | \n",
    "                                  ((data_v3['3'] <= 70) & (data_v3['3'] > 0)) | \n",
    "                                  ((data_v3['4'] <= 70) & (data_v3['4'] > 0)) | \n",
    "                                  ((data_v3['5'] <= 70) & (data_v3['5'] > 0)) | \n",
    "                                  ((data_v3['6'] <= 70) & (data_v3['6'] > 0)) | \n",
    "                                  ((data_v3['7'] <= 70) & (data_v3['7'] > 0)) | \n",
    "                                  ((data_v3['8'] <= 70) & (data_v3['8'] > 0)) | \n",
    "                                  ((data_v3['9'] <= 70) & (data_v3['9'] > 0)) ]\n",
    "            if (start_point.empty):\n",
    "                break\n",
    "            start = int(start_point.index[0])\n",
    "            end = int(start_point.index[-1])\n",
    "            if (start - 10 >= 0):\n",
    "                start -= 10\n",
    "            if (end + 10 <= 499):\n",
    "                end += 10\n",
    "            pic = cv2.resize(data_v3.loc[start: end, ['6', '1', '2', '3', '9', '8', '7', '4', '5']].values, (9, 300), interpolation =  cv2.INTER_LINEAR)\n",
    "            start_point.iloc[0:0]\n",
    "            data_v4 = pd.DataFrame(pic, columns = ['6', '1', '2', '3', '9', '8', '7', '4', '5'])\n",
    "#             print(data_v4.shape)\n",
    "#             print('------------------')\n",
    "#             print(data_v4.head(10))\n",
    "            data_v4.values.reshape((-1, 300, 9))\n",
    "            real_data_3D = data_v4.values.reshape((-1, 60, 5, 9))\n",
    "#             print(real_data_3D)\n",
    "            label = model.predict(real_data_3D)\n",
    "#             print(\"label is \", label)\n",
    "            print('label is ', np.argmax(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w5OTu8ivSyeq",
    "outputId": "686ec463-9fff-4e1e-fa71-1511ec570975"
   },
   "outputs": [],
   "source": [
    "a = input()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Model_REAL_DATASET.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
